{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: For now we only segment the liver\n",
    "- Check if we use GPU           $\\implies$  **we don't**   $\\implies$   check how to get a gpu available\n",
    "- ~~Train with dice loss.~~ **check if correct, seems weird dice was 1 - acc? trying a different dice now**\n",
    "- ~~Segmentation for the last parts.~~ (Hester) **check if correct**\n",
    "- ~~Currently we take a random voxel in the image to get a patch instead take a voxel in the liver as midpoint for a patch (Wout)~~ **check if correct**\n",
    "- Experiment with bigger patches or more patches in batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from IPython.display import clear_output\n",
    "import pickle \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "n_cores = 32\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we use gpu or cpu\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task03_Liver/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)   \n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images with label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train set as SITK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and labels, loading all takes some time, take 25 for now\n",
    "train_imgs = [sitk.ReadImage(train_instance['image']) for train_instance in train_paths[:25]]\n",
    "train_lbls = [sitk.ReadImage(train_instance['label']) for train_instance in train_paths[:25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train images as numpy\n",
    "np_train_imgs = [sitk.GetArrayFromImage(i) for i in train_imgs]\n",
    "np_train_lbls = [sitk.GetArrayFromImage(i) for i in train_lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacing\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in train_imgs:\n",
    "    print(image.GetSpacing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(np_imgs, spacings, order):\n",
    "    \"\"\"\n",
    "    Resample to 1mm x 1mm x 1mm. \n",
    "    np_imgs: list of images or labels to be resampled as numpy\n",
    "    spacings: spacings to resample with, order: (z, x, y)\n",
    "    \"\"\" \n",
    "    resampled = []\n",
    "    \n",
    "    for i in range(len(np_imgs)): \n",
    "        # apply zoom with spacing, different order for labels and imgs\n",
    "        resampled.append(zoom(np_imgs[i], spacings[i], order=order))\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store shapes before to check\n",
    "shapes_before = [img.shape for img in np_train_imgs]\n",
    "\n",
    "# spacings from sitk images\n",
    "spacings = [img.GetSpacing() for img in train_imgs]\n",
    "\n",
    "# change order\n",
    "spacings = [(z, x, y) for x, y, z in spacings]          # order: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample train images and labels, order 3 for imgs, 1 (neigherest neighbour) for labels\n",
    "np_train_imgs = resample(np_train_imgs, spacings, order=3)\n",
    "np_train_lbls = resample(np_train_lbls, spacings, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print what happened.\n",
    "print(\"Before\\t\\t\\tSpacings\\t\\tAfter\\t\\t\\tLabels\")\n",
    "for i in range(len(np_train_imgs)):    \n",
    "    # round for printing\n",
    "    spacing_round = [(round(a, 2), round(b, 2), round(c, 2)) for a, b, c in spacings]    \n",
    "    print(\"{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(shapes_before[i] , spacing_round[i], \n",
    "                                        np_train_imgs[i].shape, np_train_lbls[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resampled data as pickle and load\n",
    "I put this in the Task03_liver folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_liver.pickle', 'wb') as handle:\n",
    "    pickle.dump({'images': np_train_imgs, 'labels': np_train_lbls}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from pickle, start here if you saved the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './Task03_Liver/'\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "with open('./data_liver.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "np_train_imgs = data['images']\n",
    "np_train_lbls = data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the labels binary\n",
    "For now we will focus on only on segmentation of the liver. Set the cancer labels to liver labels. Remove this line if you want to segement cancer aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_lbls = [np.where(lbl != 2, lbl, 1) for lbl in np_train_lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we have imbalances in our data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the labels of train images\n",
    "sums = np.zeros(3)\n",
    "for lbs in np_train_lbls:\n",
    "    labels, counts = np.unique(lbs, return_counts=True)\n",
    "    \n",
    "    # if there are only 2 labels\n",
    "    if len(counts) == 2:\n",
    "        sums[:2]+=counts\n",
    "    else:\n",
    "        sums+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentages of voxels.\n",
    "total = sum(sums)\n",
    "print(\"{:.2f}% background, {:.2f}% liver, {:.2f}% cancer.\".format(sums[0]/total*100, sums[1]/total*100, sums[2]/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the np images and np labels\n",
    "In case there might be some bias in the order in which the images are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(len(np_train_imgs)))\n",
    "random.shuffle(indexes)\n",
    "\n",
    "np_train_imgs = list(np.asarray(np_train_imgs)[indexes])\n",
    "np_train_lbls = list(np.asarray(np_train_lbls)[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data set of training images, as numpy\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(np_train_imgs))\n",
    "\n",
    "train_set = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "val_set   = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. This doesn't work if the patch size doesn't fit in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, fromLiver):\n",
    "        self.patch_size = patch_size \n",
    "        self.fromLiver = fromLiver\n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # size of patch in each dimension\n",
    "        pz, px, py = self.patch_size\n",
    "        \n",
    "#         print('Patch_size: {}'.format(patch_size))\n",
    "#         print('Image_size: {}'.format(image.shape))\n",
    "\n",
    "        # pad with the min value in the image\n",
    "        min_val = np.min(image)\n",
    "        \n",
    "        # pad with half the patch size, I assume even patch size\n",
    "        padded_img = np.pad(image, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant', constant_values=min_val)\n",
    "        padded_lbl = np.pad(label, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant')\n",
    "        \n",
    "#         print('Padded_size: {}'.format(padded_img.shape))\n",
    "\n",
    "        # centre of the patch: a random point from the liver in the non padded image\n",
    "        if self.fromLiver:\n",
    "            # getting the liver labeled points\n",
    "            liver_ind = np.argwhere(label == 1)  \n",
    "            \n",
    "            # get a random point from the liver labeled points\n",
    "            r = randint(0, len(liver_ind))\n",
    "            z = liver_ind[r][0]\n",
    "            x = liver_ind[r][1]\n",
    "            y = liver_ind[r][2]\n",
    "            \n",
    "        # centre of the patch: a random location in the non padded image    \n",
    "        else:\n",
    "            dims = image.shape\n",
    "            z = randint(0, dims[0]) \n",
    "            x = randint(0, dims[1]) \n",
    "            y = randint(0, dims[2])   \n",
    "            \n",
    "        # z, x, y is the left bottom corner of the patch in the padded image (index shift with pad size)     \n",
    "        # take a patch, with the random point at the center in the padded img\n",
    "        patch  = padded_img[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "        target = padded_lbl[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (188, 188, 188)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size, fromLiver=True)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "                   overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 2)) # one-hot encoding with 2 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            # get a patch with corresponding labels from the patch extractor\n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   \n",
    "            \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2    \n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze() \n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=2)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding)(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=True):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "\n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(188, 188, 188, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "    \n",
    "    #TODO: check Conv3DTranspose correctly applied\n",
    "    \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 3 filters, # labels, softmax or ReLU?\n",
    "    finish = Conv3D(2, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (188, 188, 188)  # isotropic patch size\n",
    "target_size = (100, 100, 100)  # output size, smaller since valid convolutions are used\n",
    "batch_size  = 1                # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size, fromLiver=True)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a logger which saves the losses and saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    # logg losses and accs, add dice later\n",
    "    def __init__(self, data_dir, model_name):  \n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')        \n",
    "        self.tr_losses = []  \n",
    "#         self.tr_accs = []\n",
    "        self.val_losses = []  \n",
    "#         self.val_accs = []     \n",
    "        self.best_val_loss = float(\"inf\") \n",
    "        self.best_model = None     \n",
    "       \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # add validation info\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.val_accs.append(logs.get('val_acc')) \n",
    "        self.tr_losses.append(logs.get('loss'))\n",
    "#         self.tr_accs.append(logs.get('acc')) \n",
    "        self.plot()\n",
    "\n",
    "        # safe best model after epoch end\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('Best model saved as {}'.format(self.model_filename))\n",
    "         \n",
    "    def plot(self): \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.ylim([0, 1])\n",
    "        n = len(self.val_losses) + 1         \n",
    "        plt.plot(range(1, n), self.tr_losses, label='train loss')         \n",
    "#         plt.plot(range(1, n), self.tr_accs, label='train accuracy')\n",
    "        plt.plot(range(1, n), self.val_losses, label='val loss')        \n",
    "#         plt.plot(range(1, n), self.val_accs, label='val accuracy') \n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data dir to store best model\n",
    "print(os.getcwd())\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to inline, I don't know how to plot in notebook mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "Dice loss seems to be a good pick for 3D segmentation with class inbalances. We have to look if this works like this.\n",
    "In our case y_pred is output of the softmax.(see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "\n",
    "$$ \\textbf{Dice loss} =  1 - \\frac{2 \\hspace{0.3em}|X \\cap Y|}{|X|+ |Y|} $$\n",
    "\n",
    "\n",
    "**For binary volumes of N voxels (Milletari et al., 2016, VNet):**\n",
    "\n",
    "$$ \\textbf{Dice loss} = 1 -\\frac{2 \\sum_{i}^{N} p_{i} g_{i}}{\\sum_{i}^{N} p_{i}^{2}+\\sum_{i}^{N} g_{i}^{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss as above\n",
    "def dice_loss_bv(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    G = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    return 1. - (2. * K.sum(P * G) + epsilon) / (K.sum(P**2.) + K.sum(G**2.) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed in Milletari et al. [8] as a loss function, the 2-class variant of the Dice loss, denoted DL2, can be expressed as**\n",
    "\n",
    "$$\n",
    "\\mathrm{DL}_{2}=1-\\frac{\\sum_{n=1}^{N} p_{n} r_{n}+\\epsilon}{\\sum_{n=1}^{N} p_{n}+r_{n}+\\epsilon}-\\frac{\\sum_{n=1}^{N}\\left(1-p_{n}\\right)\\left(1-r_{n}\\right)+\\epsilon}{\\sum_{n=1}^{N} 2-p_{n}-r_{n}+\\epsilon}\n",
    "$$\n",
    "\n",
    "Sudre, Carole H., et al. \"Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.\" https://arxiv.org/pdf/1707.03237.pdf\n",
    "\n",
    "\n",
    "**Let R be the reference foreground segmentation\n",
    "(gold standard) with voxel values $r_n$, and P the predicted probabilistic map for the foreground label over N image elements $p_n$, with the background class probability being 1 − P. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation in a binary classification (foreground vs. background) formulation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    R = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    a = K.sum(P * R) + epsilon\n",
    "    b = K.sum(P + R) + epsilon\n",
    "    c = K.sum((1 - P) * (1 - R)) + epsilon \n",
    "    d = K.sum((2 - P - R)) + epsilon\n",
    "    \n",
    "    return 1 - a/b - c/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 10**-4\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "metrics         = ['accuracy']  # probably not useful (?)\n",
    "steps_per_epoch = 10\n",
    "epochs          = 10\n",
    "logger          = Logger(data_dir, '3D-UNet-20-05')\n",
    "\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# compile model\n",
    "unet_3d.compile(optimizer=optimizer, loss=dice_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "unet_3d.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=image_generator_val,\n",
    "                    verbose=1,\n",
    "                    validation_steps=10,\n",
    "                    callbacks=[logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(image, patch_size, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    image       : the input image (as numpy)\n",
    "    patch_size  : patch_size of the input for the UNet\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    z, y, x = patch_size\n",
    "    \n",
    "    # pad with min value from image, always safe\n",
    "    min_val = np.min(image)\n",
    "    \n",
    "    # size of padding for each dimension\n",
    "    pad_z = (z - target_size[0]) // 2\n",
    "    pad_x = (x - target_size[1]) // 2\n",
    "    pad_y = (y - target_size[2]) // 2\n",
    "    \n",
    "    # pad with a tuple for how much on each side for every dimension\n",
    "    padded_input = np.pad(image, ((pad_z, pad_z), (pad_x, pad_x), (pad_y, pad_y)), 'constant', constant_values=min_val)\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    image       : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "    image = np.clip(image, -1000, 3000)\n",
    "    image = (image - np.min(image)) / np.ptp(image)    \n",
    "    \n",
    "    # pad the input image:\n",
    "    pad_img = padding(image, patch_size, target_size)  \n",
    "\n",
    "    print(\"Image size: {}\".format(image.shape))\n",
    "    print(\"Padded image size: {}\".format(pad_img.shape))\n",
    "    \n",
    "    dims = image.shape\n",
    "    # how many times target size fits in a dimension \n",
    "    pz = dims[0] // target_size[0] \n",
    "    px = dims[1] // target_size[1] \n",
    "    py = dims[2] // target_size[2] \n",
    "    \n",
    "    # segmentation map, same size as input image\n",
    "    segmentation = np.zeros(image.shape)   \n",
    "    \n",
    "    for z in range(pz, -1, -1):         \n",
    "        for x in range(px, -1, -1):\n",
    "            for y in range(py, -1, -1):  \n",
    "                \n",
    "                # shift starting point with target_size\n",
    "                start_z = z * target_size[0]\n",
    "                start_x = x * target_size[1]\n",
    "                start_y = y * target_size[2]\n",
    "                \n",
    "                # if the patch does not fit:\n",
    "                if start_z + patch_size[0] > pad_img.shape[0]:\n",
    "                    start_z = pad_img.shape[0] - patch_size[0]\n",
    "                if start_x + patch_size[1] > pad_img.shape[1]:\n",
    "                    start_x = pad_img.shape[1] - patch_size[1]\n",
    "                if start_y + patch_size[2] > pad_img.shape[2]:\n",
    "                    start_y = pad_img.shape[2] - patch_size[2]\n",
    "                \n",
    "                # Get patch: shift with target_size, take patch_size                \n",
    "                patch = pad_img[start_z:start_z + patch_size[0], \n",
    "                                start_x:start_x + patch_size[1], \n",
    "                                start_y:start_y + patch_size[2]]     \n",
    "\n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = model.predict(patch)\n",
    "\n",
    "                # Put the prediction in segmentation map, shift with target_size, take target_size\n",
    "                segmentation[start_z:start_z + target_size[0], \n",
    "                             start_x:start_x + target_size[1], \n",
    "                             start_y:start_y + target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "    \n",
    "    return segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an image and a label from train set\n",
    "image = val_set.imgs[0][200:400, :, -200:]\n",
    "label = val_set.lbls[0][200:400, :, -200:]\n",
    "patch_size  = (188, 188, 188)  \n",
    "target_size = (100, 100, 100)    \n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "unet_3d = load_model(os.path.join(data_dir, '3D_UNet' + '.h5'), custom_objects={'soft_dice_loss_f': soft_dice_loss_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = predict_image_segmentation(unet_3d, image, target_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(segmentation, return_counts=True))\n",
    "print(np.unique(label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.imshow(segmentation[150, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show segmentation\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=segmentation, overlay_1_thres=1, \n",
    "                   overlay_2=segmentation, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=label, overlay_1_thres=1, \n",
    "                   overlay_2=label, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
