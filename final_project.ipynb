{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "n_cores = 32\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task03_Liver/'\n",
    "print([file for file in os.listdir(data_path) if not file.startswith('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)\n",
    "    print(d.keys())\n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images with label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some stuff about the dataset\n",
    "print(d['tensorImageSize'])\n",
    "print(d['modality'])\n",
    "print(d['labels'])\n",
    "print(d['numTraining'])\n",
    "print(d['numTest']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train set as SITK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and labels, loading all takes some time, so just take 5 for now\n",
    "train_imgs = [sitk.ReadImage(train_instance['image']) for train_instance in train_paths[100:105]]\n",
    "train_lbls = [sitk.ReadImage(train_instance['label']) for train_instance in train_paths[100:105]]\n",
    "\n",
    "for img in train_imgs:\n",
    "    print(img.GetSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train images as numpy\n",
    "np_train_imgs = [sitk.GetArrayFromImage(i) for i in train_imgs]\n",
    "np_train_lbls = [sitk.GetArrayFromImage(i) for i in train_lbls]\n",
    "\n",
    "for img in np_train_imgs:\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacing\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in train_imgs:\n",
    "    print(image.GetSpacing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(np_imgs, spacings):\n",
    "    \"\"\"\n",
    "    Resample to 1mm x 1mm x 1mm. \n",
    "    np_imgs: list of images or labels to be resampled as numpy\n",
    "    spacings: spacings to resample with, order: (z, x, y)\n",
    "    \"\"\" \n",
    "    resampled = []\n",
    "    \n",
    "    for i in range(len(np_imgs)): \n",
    "        # apply zoom with spacing, no clue what spline interpolation is\n",
    "        resampled.append(zoom(np_imgs[i], spacings[i], order=1))\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store shapes before to check\n",
    "shapes_before = [img.shape for img in np_train_imgs]\n",
    "\n",
    "# spacings from sitk images\n",
    "spacings = [img.GetSpacing() for img in train_imgs]\n",
    "\n",
    "# change order\n",
    "spacings = [(z, x, y) for x, y, z in spacings]          # order: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample train images and labels\n",
    "np_train_imgs = resample(np_train_imgs, spacings)\n",
    "np_train_lbls = resample(np_train_lbls, spacings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print what happened.\n",
    "print(\"Before\\t\\t\\tSpacings\\t\\tAfter\")\n",
    "for i in range(len(np_train_imgs)):    \n",
    "    # round for printing\n",
    "    spacing_round = [(round(a, 1), round(b, 1), round(c, 1)) for a, b, c in spacings]    \n",
    "    print(\"{}\\t\\t{}\\t\\t{}\".format(shapes_before[i] , spacing_round[i], np_train_imgs[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for datasets\n",
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data set of training images, as numpy\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(train_imgs))\n",
    "\n",
    "train_set = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "val_set   = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])\n",
    "#train_set.show_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. This doesn't work if the patch size doesn't fit in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size):\n",
    "        self.patch_size = patch_size \n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # size of patch in each dimension\n",
    "        pz = self.patch_size[0]\n",
    "        px = self.patch_size[1]\n",
    "        py = self.patch_size[2]  \n",
    "        \n",
    "#         print('Patch_size: {}'.format(patch_size))\n",
    "#         print('Image_size: {}'.format(image.shape))\n",
    "\n",
    "        # pad with the min value in the image\n",
    "        min_val = np.min(image)\n",
    "        \n",
    "        # pad with half the patch size, I assume even patch size\n",
    "        padded_img = np.pad(image, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant', constant_values=min_val)\n",
    "        padded_lbl = np.pad(label, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant')\n",
    "        \n",
    "#         print('Padded_size: {}'.format(padded_img.shape))\n",
    "        \n",
    "        # pick a random location\n",
    "        # subtract half the patch size (in every dimension) to get the bottom left corner of a patch\n",
    "        dims = image.shape           \n",
    "        r = randint(0, dims[0]) - pz//2\n",
    "        c = randint(0, dims[1]) - px//2\n",
    "        d = randint(0, dims[2]) - py//2\n",
    "        \n",
    "        # location of the point in the padded image\n",
    "        z = r + pz//2\n",
    "        x = c + px//2\n",
    "        y = d + py//2\n",
    "\n",
    "        # take a patch, with the random point at the center in the padded img\n",
    "        patch  = padded_img[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "        target = padded_lbl[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (132, 132, 132)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "                   overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel (??)\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 3)) # one-hot encoding with 3 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   # when image size is equal to patch size, this line is useless...\n",
    "        \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2       # // : floor division\n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze() \n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=3)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding)(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=True):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "\n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(132, 132, 132, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "    \n",
    "    #TODO: check Conv3DTranspose correctly applied\n",
    "    \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 3 filters, # labels, softmax or ReLU?\n",
    "    finish = Conv3D(3, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure how this still isn't exactly the same parameters as the paper (19,069,955). Close enough tho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (132, 132, 132)  # isotropic patch size\n",
    "target_size = (44, 44, 44)     # output size, smaller since valid convolutions are used\n",
    "batch_size = 1                 # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a logger which saves the losses and saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, data_dir, model_name):   \n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')\n",
    "        \n",
    "        self.tr_losses = []  \n",
    "        self.tr_accs = []\n",
    "        self.val_losses = []  \n",
    "        self.val_accs = []     \n",
    "        self.best_val_loss = float(\"inf\") \n",
    "        self.best_model = None\n",
    "        \n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            print(\"hallo\")            \n",
    "            self.tr_losses.append(logs.get('loss'))\n",
    "            self.tr_accs.append(logs.get('acc'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.val_accs.append(logs.get('val_acc'))\n",
    "            \n",
    "        def on_epoch_end(self, batch, logs={}):     \n",
    "            if self.val_losses[-1] < min(self.val_losses):\n",
    "                self.val_losses = self.val_losses[-1]\n",
    "                self.model.save(self.model_filename) # save best model to disk\n",
    "                print('Best model saved as {}'.format(self.model_filename))\n",
    "            self.plot()   \n",
    "        \n",
    "        def plot(self):\n",
    "            print(\"hallo\")\n",
    "            #clear_output()\n",
    "            N = len(self.losses)\n",
    "            plt.figure(figsize=(50, 10))            \n",
    "            plt.plot(range(0, N), self.tr_losses, label='train loss')         \n",
    "            plt.plot(range(0, N), self.tr_accs, label='train acc')\n",
    "            plt.plot(range(0, N), self.val_losses, label='val loss')        \n",
    "            plt.plot(range(0, N), self.val_accs, label='val acc') \n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data dir to store best model\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "# optimizer       = SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "loss            = 'categorical_crossentropy'\n",
    "metrics         = ['accuracy']  # probably not useful (?)\n",
    "steps_per_epoch = 5\n",
    "epochs          = 5\n",
    "logger          = Logger(data_dir, '3D_UNet')\n",
    "\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# compile model\n",
    "unet_3d.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# train the model\n",
    "unet_3d.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    validation_data=image_generator_val,\n",
    "                    validation_steps=1,\n",
    "                    callbacks=[logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how training went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training info\n",
    "plt.figure()\n",
    "plt.plot(range(epochs), hist.history['loss'], label='train loss')\n",
    "plt.plot(range(epochs), hist.history['acc'], label='train acc')\n",
    "plt.plot(range(epochs), hist.history['val_loss'], label='val loss')\n",
    "plt.plot(range(epochs), hist.history['val_acc'], label='val acc')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(input_image, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    input_image : the input image (as numpy)\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    \n",
    "    padded_input = -1.0 * np.ones(target_size).astype(float)\n",
    "    output_size = input_image.shape\n",
    "    D = ((target_size[0]-output_size[0])//2, (target_size[1]-output_size[1])//2, (target_size[2]-output_size[2])//2)\n",
    "    padded_input[D[0]:D[0]+output_size[0], D[1]:D[1]+output_size[1], D[2]:D[2]+output_size[2]] = input_image\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, input_image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    input_image : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    dims = input_image.shape\n",
    "    segmentation = np.zeros((dims[0], dims[1], dims[2])) \n",
    "    \n",
    "    # first do the ones where we will use the full pred, for the last one we are gonna need to crop.\n",
    "    # how many times target size fits in a dimension\n",
    "    pz = dims[0] // target_size[0]\n",
    "    px = dims[1] // target_size[1]\n",
    "    py = dims[2] // target_size[2]\n",
    "    \n",
    "    # Pad the input image:\n",
    "    total_padding = (2 * (patch_size[0]-target_size[0]), 2 * (patch_size[1]-target_size[1]), 2 * (patch_size[2]-target_size[2]))\n",
    "    pad_image = padding(input_image, (dims[0]+total_padding[0], dims[1]+total_padding[1], dims[2]+total_padding[2]))\n",
    "    \n",
    "    for z in range(pz):         \n",
    "        for x in range(px):\n",
    "            for y in range(py):  \n",
    "                \n",
    "                # shift starting point with target_size\n",
    "                start_z = z * target_size[0]\n",
    "                start_x = x * target_size[1]\n",
    "                start_y = y * target_size[2]\n",
    "                \n",
    "                # Get patch: shift with target_size, take patch_size\n",
    "                print((start_z, start_x, start_y))\n",
    "                patch = pad_image[start_z:start_z + patch_size[0], \n",
    "                                  start_x:start_x + patch_size[1], \n",
    "                                  start_y:start_y + patch_size[2]]     \n",
    "                \n",
    "                # Do something with the parts that did not fit: ... \n",
    "                \n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = model.predict(patch)\n",
    "                \n",
    "                # Put the prediction in segmentation map, shift with target_size, take target_size\n",
    "                segmentation[start_z:start_z + target_size[0], \n",
    "                             start_x:start_x + target_size[1], \n",
    "                             start_y:start_y + target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "        \n",
    "    return segmentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an image and a label from train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "patch_size  = (132, 132, 132)  # isotropic patch size\n",
    "target_size = (44, 44, 44)    \n",
    "print(image.shape)\n",
    "print(target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = predict_image_segmentation(unet_3d, image, target_size, patch_size)\n",
    "print(segmentation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(segmentation, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show segmentation\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=segmentation, overlay_1_thres=1, \n",
    "                   overlay_2=segmentation, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=label, overlay_1_thres=1, \n",
    "                   overlay_2=label, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this stuff later:\n",
    "* Train network and plot loss, including loss on validation data on epoch end and saving the best model. (see keras.callbacks.Callback)\n",
    "* Add dice loss (note that we do not have binary labels at the moment).\n",
    "* Should we shuffle the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
