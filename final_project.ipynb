{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "n_cores = 32\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imagesTr', 'dataset.json', 'labelsTr', 'imagesTs']\n"
     ]
    }
   ],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task03_Liver/'\n",
    "print([file for file in os.listdir(data_path) if not file.startswith('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'reference', 'training', 'description', 'numTest', 'labels', 'release', 'licence', 'tensorImageSize', 'numTraining', 'modality', 'name'])\n"
     ]
    }
   ],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)\n",
    "    print(d.keys())\n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images with label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D\n",
      "{'0': 'CT'}\n",
      "{'2': 'cancer', '1': 'liver', '0': 'background'}\n",
      "131\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# print some stuff about the dataset\n",
    "print(d['tensorImageSize'])\n",
    "print(d['modality'])\n",
    "print(d['labels'])\n",
    "print(d['numTraining'])\n",
    "print(d['numTest']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home3/hesterh/ISMI_project/Task03_Liver\n"
     ]
    }
   ],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train set as SITK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 333)\n",
      "(512, 512, 987)\n",
      "(512, 512, 188)\n",
      "(512, 512, 761)\n",
      "(512, 512, 122)\n"
     ]
    }
   ],
   "source": [
    "# load images and labels, loading all takes some time, so just take 5 for now\n",
    "train_imgs = [sitk.ReadImage(train_instance['image']) for train_instance in train_paths[100:105]]\n",
    "train_lbls = [sitk.ReadImage(train_instance['label']) for train_instance in train_paths[100:105]]\n",
    "\n",
    "for img in train_imgs:\n",
    "    print(img.GetSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 512, 512)\n",
      "(987, 512, 512)\n",
      "(188, 512, 512)\n",
      "(761, 512, 512)\n",
      "(122, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# train images as numpy\n",
    "np_train_imgs = [sitk.GetArrayFromImage(i) for i in train_imgs]\n",
    "np_train_lbls = [sitk.GetArrayFromImage(i) for i in train_lbls]\n",
    "\n",
    "for img in np_train_imgs:\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacing\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5976560115814209, 0.5976560115814209, 1.25)\n",
      "(0.771484375, 0.771484375, 0.699999988079071)\n",
      "(0.6445310115814209, 0.6445310115814209, 2.0)\n",
      "(0.7820000052452087, 0.7820000052452087, 0.800000011920929)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for image in train_imgs:\n",
    "    print(image.GetSpacing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(np_imgs, spacings):\n",
    "    \"\"\"\n",
    "    Resample to 1mm x 1mm x 1mm. \n",
    "    np_imgs: list of images or labels to be resampled as numpy\n",
    "    spacings: spacings to resample with, order: (z, x, y)\n",
    "    \"\"\" \n",
    "    resampled = []\n",
    "    \n",
    "    for i in range(len(np_imgs)): \n",
    "        # apply zoom with spacing, no clue what spline interpolation is\n",
    "        resampled.append(zoom(np_imgs[i], spacings[i], order=1))\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store shapes before to check\n",
    "shapes_before = [img.shape for img in np_train_imgs]\n",
    "\n",
    "# spacings from sitk images\n",
    "spacings = [img.GetSpacing() for img in train_imgs]\n",
    "\n",
    "# change order\n",
    "spacings = [(z, x, y) for x, y, z in spacings]          # order: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/scipy/ndimage/interpolation.py:616: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# resample train images and labels\n",
    "np_train_imgs = resample(np_train_imgs, spacings)\n",
    "np_train_lbls = resample(np_train_lbls, spacings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\t\t\tSpacings\t\tAfter\n",
      "(333, 512, 512)\t\t(1.2, 0.6, 0.6)\t\t(416, 306, 306)\n",
      "(987, 512, 512)\t\t(0.7, 0.8, 0.8)\t\t(691, 395, 395)\n",
      "(188, 512, 512)\t\t(2.0, 0.6, 0.6)\t\t(376, 330, 330)\n",
      "(761, 512, 512)\t\t(0.8, 0.8, 0.8)\t\t(609, 400, 400)\n",
      "(122, 512, 512)\t\t(1.0, 1.0, 1.0)\t\t(122, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# lets print what happened.\n",
    "print(\"Before\\t\\t\\tSpacings\\t\\tAfter\")\n",
    "for i in range(len(np_train_imgs)):    \n",
    "    # round for printing\n",
    "    spacing_round = [(round(a, 1), round(b, 1), round(c, 1)) for a, b, c in spacings]    \n",
    "    print(\"{}\\t\\t{}\\t\\t{}\".format(shapes_before[i] , spacing_round[i], np_train_imgs[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for datasets\n",
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data set of training images, as numpy\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(train_imgs))\n",
    "\n",
    "train_set = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "val_set   = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])\n",
    "#train_set.show_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. This doesn't work if the patch size doesn't fit in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size):\n",
    "        self.patch_size = patch_size \n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        pz = self.patch_size[0]\n",
    "        px = self.patch_size[1]\n",
    "        py = self.patch_size[2]\n",
    "        \n",
    "        # pick a random location, from which we can get a patch with patch_size\n",
    "        # subtract half the patch size (in every dimension) to get the bottom left corner of a patch\n",
    "        dims = image.shape           \n",
    "        r = randint(0, dims[0]) - 0.5 * pz\n",
    "        c = randint(0, dims[1]) - 0.5 * px\n",
    "        d = randint(0, dims[2]) - 0.5 * py\n",
    "        \n",
    "        # take a patch, with the random point at the center\n",
    "        patch  = image[r:r+pz, c:c+px, d:d+py].reshape(pz, px, px, 1)\n",
    "        target = label[r:r+pz, c:c+px, d:d+py].reshape(pz, px, px, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-24b60268bcf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# lets check some patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72f8c3068a13>\u001b[0m in \u001b[0;36mget_patch\u001b[0;34m(self, image, label)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# take a patch, with the random point at the center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpatch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (132, 132, 132)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "# plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "# multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "#                    overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel (??)\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 3)) # one-hot encoding with 3 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   # when image size is equal to patch size, this line is useless...\n",
    "        \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2       # // : floor division\n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze() \n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=3)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding)(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=True):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "\n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(132, 132, 132, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "    \n",
    "    #TODO: check Conv3DTranspose correctly applied\n",
    "    \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 3 filters, # labels, softmax or ReLU?\n",
    "    finish = Conv3D(3, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             (None, 132, 132, 132, 1)         0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)                                (None, 130, 130, 130, 32)        896               input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization)       (None, 130, 130, 130, 32)        128               conv3d_1[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)                                (None, 128, 128, 128, 64)        55360             batch_normalization_1[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization)       (None, 128, 128, 128, 64)        256               conv3d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)                   (None, 64, 64, 64, 64)           0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)                                (None, 62, 62, 62, 64)           110656            max_pooling3d_1[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization)       (None, 62, 62, 62, 64)           256               conv3d_3[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)                                (None, 60, 60, 60, 128)          221312            batch_normalization_3[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization)       (None, 60, 60, 60, 128)          512               conv3d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)                   (None, 30, 30, 30, 128)          0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)                                (None, 28, 28, 28, 128)          442496            max_pooling3d_2[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization)       (None, 28, 28, 28, 128)          512               conv3d_5[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)                                (None, 26, 26, 26, 256)          884992            batch_normalization_5[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization)       (None, 26, 26, 26, 256)          1024              conv3d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)                   (None, 13, 13, 13, 256)          0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)                                (None, 11, 11, 11, 256)          1769728           max_pooling3d_3[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNormalization)       (None, 11, 11, 11, 256)          1024              conv3d_7[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)                                (None, 9, 9, 9, 512)             3539456           batch_normalization_7[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNormalization)       (None, 9, 9, 9, 512)             2048              conv3d_8[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_1 (Cropping3D)                        (None, 18, 18, 18, 256)          0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTranspose)             (None, 18, 18, 18, 512)          2097664           batch_normalization_8[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)                      (None, 18, 18, 18, 768)          0                 cropping3d_1[0][0]                                \n",
      "                                                                                                    conv3d_transpose_1[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)                                (None, 16, 16, 16, 256)          5308672           concatenate_1[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNormalization)       (None, 16, 16, 16, 256)          1024              conv3d_9[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)                               (None, 14, 14, 14, 256)          1769728           batch_normalization_9[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNormalization)      (None, 14, 14, 14, 256)          1024              conv3d_10[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_2 (Cropping3D)                        (None, 28, 28, 28, 128)          0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTranspose)             (None, 28, 28, 28, 256)          524544            batch_normalization_10[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)                      (None, 28, 28, 28, 384)          0                 cropping3d_2[0][0]                                \n",
      "                                                                                                    conv3d_transpose_2[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)                               (None, 26, 26, 26, 128)          1327232           concatenate_2[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNormalization)      (None, 26, 26, 26, 128)          512               conv3d_11[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)                               (None, 24, 24, 24, 128)          442496            batch_normalization_11[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNormalization)      (None, 24, 24, 24, 128)          512               conv3d_12[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_3 (Cropping3D)                        (None, 48, 48, 48, 64)           0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTranspose)             (None, 48, 48, 48, 128)          131200            batch_normalization_12[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)                      (None, 48, 48, 48, 192)          0                 cropping3d_3[0][0]                                \n",
      "                                                                                                    conv3d_transpose_3[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)                               (None, 46, 46, 46, 64)           331840            concatenate_3[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNormalization)      (None, 46, 46, 46, 64)           256               conv3d_13[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)                               (None, 44, 44, 44, 64)           110656            batch_normalization_13[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNormalization)      (None, 44, 44, 44, 64)           256               conv3d_14[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)                               (None, 44, 44, 44, 3)            195               batch_normalization_14[0][0]                      \n",
      "======================================================================================================================================================\n",
      "Total params: 19,078,467\n",
      "Trainable params: 19,073,795\n",
      "Non-trainable params: 4,672\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure how this still isn't exactly the same parameters as the paper (19,069,955). Close enough tho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (132, 132, 132)  # isotropic patch size\n",
    "target_size = (44, 44, 44)     # output size, smaller since valid convolutions are used\n",
    "batch_size = 1                 # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_generator_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d37bab6e32a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimage_generator_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mimage_generator_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_generator_train' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate   = 0.001\n",
    "# optimizer       = SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "loss            = 'categorical_crossentropy'\n",
    "metrics         = ['accuracy']  # probably not useful (?)\n",
    "steps_per_epoch = 10\n",
    "epochs          = 10\n",
    "\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# compile model\n",
    "unet_3d.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# train the model\n",
    "hist = unet_3d.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    validation_data=image_generator_val,\n",
    "                    validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This happens. because it patch size doesn't always fit. So we are going to have to modify the patch extractor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how training went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training info\n",
    "plt.figure()\n",
    "plt.plot(range(epochs), hist.history['loss'], label='train loss')\n",
    "plt.plot(range(epochs), hist.history['acc'], label='train acc')\n",
    "plt.plot(range(epochs), hist.history['val_loss'], label='val loss')\n",
    "plt.plot(range(epochs), hist.history['val_acc'], label='val acc')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(input_image, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    input_image : the input image (as numpy)\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    \n",
    "    padded_input = -1.0 * np.ones(target_size).astype(float)\n",
    "    output_size = input_image.shape\n",
    "    D = ((target_size[0]-output_size[0])//2, (target_size[1]-output_size[1])//2, (target_size[2]-output_size[2])//2)\n",
    "    padded_input[D[0]:D[0]+output_size[0], D[1]:D[1]+output_size[1], D[2]:D[2]+output_size[2]] = input_image\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, input_image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    input_image : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    dims = input_image.shape\n",
    "    segmentation = np.zeros((dims[0], dims[1], dims[2])) \n",
    "    \n",
    "    # first do the ones where we will use the full pred, for the last one we are gonna need to crop.\n",
    "    # how many times target size fits in a dimension\n",
    "    pz = dims[0] // target_size[0]\n",
    "    px = dims[1] // target_size[1]\n",
    "    py = dims[2] // target_size[2]\n",
    "    \n",
    "    # Pad the input image:\n",
    "    total_padding = (2 * (patch_size[0]-target_size[0]), 2 * (patch_size[1]-target_size[1]), 2 * (patch_size[2]-target_size[2]))\n",
    "    pad_image = padding(input_image, (dims[0]+total_padding[0], dims[1]+total_padding[1], dims[2]+total_padding[2]))\n",
    "\n",
    "    # Tuple with positions of beginning of a patch:\n",
    "    xyz_loc = (0, 0, 0)\n",
    "\n",
    "    for z in np.linspace(0, pz * target_size[0], pz): \n",
    "        for x in np.linspace(0, px * target_size[1], px):\n",
    "            for y in np.linspace(0, py * target_size[2], py):\n",
    "        \n",
    "                # Get patch:\n",
    "                patch = pad_image[xyz_loc[0]:xyz_loc[0]+patch_size[0], \n",
    "                                  xyz_loc[1]:xyz_loc[1]+patch_size[1], \n",
    "                                  xyz_loc[2]:xyz_loc[2]+patch_size[2]]\n",
    "               \n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = unet.predict(patch)\n",
    "                segmentation[z:z+target_size[0], \n",
    "                                x:x+target_size[1], \n",
    "                                y:y+target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "\n",
    "        # Keep track of patch positions:\n",
    "                xyz_loc = (0, 0, xyz_loc[2]+target_size[2])\n",
    "            xyz_loc = (0, xyz_loc[1]+target_size[1], 0)\n",
    "        xyz_loc = (xyz_loc[0]+target_size[0], 0, 0) \n",
    "        \n",
    "                \n",
    "    # Do something with the parts that did not fit: ... \n",
    "    \n",
    "    return segmentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 306, 306)\n",
      "(44, 44, 44)\n"
     ]
    }
   ],
   "source": [
    "# take an image and a label from train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "patch_size  = (132, 132, 132)  # isotropic patch size\n",
    "target_size = (44, 44, 44)    \n",
    "print(image.shape)\n",
    "print(target_size)\n",
    "ones_image = np.ones(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(132, 132, 132)\n",
      "(416, 306, 306)\n"
     ]
    }
   ],
   "source": [
    "segmentation = predict_image_segmentation(unet_3d, image, target_size, patch_size)\n",
    "print(segmentation.shape)\n",
    "# plt.imshow(segmentation[250, :, :])\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "1. ~~Setting the spacing should not effect width. Resampling to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2), you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm.  As a tip, look for “scipy zoom”.~~ **(Done, but is it correct like this?)**\n",
    "2. ~~Choose a patch size that is also isotropic because otherwise you would include less information / context in some direction.~~\n",
    "3. Modifiy the patch extractor that when the patch size doesn't fit we pad the lowest occuring value for missing data. (only for a few cases, might not be a disaster). I think Niko also means we shouldn't randomly extract patches for training the model, but extract patches in the same way we would for a full segmentation (which also makes sense).\n",
    "4. Find good patch_size (+ padded_size) for images -> full segmentation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this stuff later:\n",
    "* Train network and plot loss, including loss on validation data on epoch end and saving the best model. (see keras.callbacks.Callback)\n",
    "* Add dice loss (note that we do not have binary labels at the moment).\n",
    "* Should we shuffle the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
