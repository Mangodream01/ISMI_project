{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: For now we only segment the liver\n",
    "- Added shuffle for data, there might have been some bias in the order.\n",
    "- Should we use all the training data? (doesn't fit in memory at the moment)\n",
    "- Should we validate with a \"real\" dice (also done on in the competiton evaluation)? With patches or full images?\n",
    "- Experiment with bigger patches or more patches in batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from IPython.display import clear_output\n",
    "import pickle \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "n_cores = 16\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8270264475881289365\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11321648743\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13206431668856760341\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11321648743\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 10523617580257072059\n",
      "physical_device_desc: \"device: 1, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5\"\n",
      "]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if we use gpu or cpu\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "assert len(K.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task03_Liver/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)   \n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images with label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train set as SITK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and labels, loading all takes some time, take 50 for now\n",
    "train_imgs = [sitk.ReadImage(train_instance['image']) for train_instance in train_paths[50:100]]\n",
    "train_lbls = [sitk.ReadImage(train_instance['label']) for train_instance in train_paths[50:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train images as numpy\n",
    "np_train_imgs = [sitk.GetArrayFromImage(i) for i in train_imgs]\n",
    "np_train_lbls = [sitk.GetArrayFromImage(i) for i in train_lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacing\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in train_imgs:\n",
    "    print(image.GetSpacing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(np_imgs, spacings, order):\n",
    "    \"\"\"\n",
    "    Resample to 1mm x 1mm x 1mm. \n",
    "    np_imgs: list of images or labels to be resampled as numpy\n",
    "    spacings: spacings to resample with, order: (z, x, y)\n",
    "    \"\"\" \n",
    "    resampled = []\n",
    "    \n",
    "    for i in range(len(np_imgs)): \n",
    "        # apply zoom with spacing, different order for labels and imgs\n",
    "        resampled.append(zoom(np_imgs[i], spacings[i], order=order))\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store shapes before to check\n",
    "shapes_before = [img.shape for img in np_train_imgs]\n",
    "\n",
    "# spacings from sitk images\n",
    "spacings = [img.GetSpacing() for img in train_imgs]\n",
    "\n",
    "# change order\n",
    "spacings = [(z, x, y) for x, y, z in spacings]          # order: (z, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample train images and labels, order 3 for imgs, 1 (neigherest neighbour) for labels\n",
    "np_train_imgs = resample(np_train_imgs, spacings, order=3)\n",
    "np_train_lbls = resample(np_train_lbls, spacings, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print what happened.\n",
    "print(\"Before\\t\\t\\tSpacings\\t\\tAfter\\t\\t\\tLabels\")\n",
    "for i in range(len(np_train_imgs)):    \n",
    "    # round for printing\n",
    "    spacing_round = [(round(a, 1), round(b, 1), round(c, 1)) for a, b, c in spacings]    \n",
    "    print(\"{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(shapes_before[i] , spacing_round[i], \n",
    "                                        np_train_imgs[i].shape, np_train_lbls[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resampled data as pickle and load\n",
    "I put this in the Task03_liver folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_liver.pickle', 'wb') as handle:\n",
    "    pickle.dump({'images': np_train_imgs, 'labels': np_train_lbls}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from pickle, start here if you saved the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home4/mbotros/ISMI_project/Task03_Liver\n"
     ]
    }
   ],
   "source": [
    "data_path = './Task03_Liver/'\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "with open('./data_liver.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "np_train_imgs = data['images']\n",
    "np_train_lbls = data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the labels binary\n",
    "For now we will focus on only on segmentation of the liver. Set the cancer labels to liver labels. Remove this line if you want to segment cancer aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_lbls = [np.where(lbl != 2, lbl, 1) for lbl in np_train_lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we have imbalances in our data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the labels of train images\n",
    "sums = np.zeros(3)\n",
    "for lbs in np_train_lbls:\n",
    "    labels, counts = np.unique(lbs, return_counts=True)\n",
    "    \n",
    "    # if there are only 2 labels\n",
    "    if len(counts) == 2:\n",
    "        sums[:2]+=counts\n",
    "    else:\n",
    "        sums+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.02% background, 1.98% liver, 0.00% cancer.\n"
     ]
    }
   ],
   "source": [
    "# print percentages of voxels.\n",
    "total = sum(sums)\n",
    "print(\"{:.2f}% background, {:.2f}% liver, {:.2f}% cancer.\".format(sums[0]/total*100, sums[1]/total*100, sums[2]/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the np images and np labels\n",
    "In case there might be some bias in the order in which the images are stored. The images seem already shuffled so lets skip this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = list(range(len(np_train_imgs)))\n",
    "# random.shuffle(indexes)\n",
    "\n",
    "# np_train_imgs = list(np.asarray(np_train_imgs)[indexes])\n",
    "# np_train_lbls = list(np.asarray(np_train_lbls)[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data set of training images, as numpy\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(np_train_imgs))\n",
    "\n",
    "train_set = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "val_set   = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. This doesn't work if the patch size doesn't fit in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, fromLiver):\n",
    "        self.patch_size = patch_size \n",
    "        self.fromLiver = fromLiver\n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # size of patch in each dimension\n",
    "        pz, px, py = self.patch_size\n",
    "        \n",
    "#         print('Patch_size: {}'.format(patch_size))\n",
    "#         print('Image_size: {}'.format(image.shape))\n",
    "\n",
    "        # pad with the min value in the image\n",
    "        min_val = np.min(image)\n",
    "        \n",
    "        # pad with half the patch size, I assume even patch size\n",
    "        padded_img = np.pad(image, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant', constant_values=min_val)\n",
    "        padded_lbl = np.pad(label, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant')\n",
    "        \n",
    "#         print('Padded_size: {}'.format(padded_img.shape))\n",
    "\n",
    "        # centre of the patch: a random point from the liver in the non padded image\n",
    "        if self.fromLiver:\n",
    "            # getting the liver labeled points\n",
    "            liver_ind = np.argwhere(label == 1)  \n",
    "            \n",
    "            # get a random point from the liver labeled points\n",
    "            r = randint(0, len(liver_ind))\n",
    "            z = liver_ind[r][0]\n",
    "            x = liver_ind[r][1]\n",
    "            y = liver_ind[r][2]\n",
    "            \n",
    "        # centre of the patch: a random location in the non padded image    \n",
    "        else:\n",
    "            dims = image.shape\n",
    "            z = randint(0, dims[0]) \n",
    "            x = randint(0, dims[1]) \n",
    "            y = randint(0, dims[2])   \n",
    "            \n",
    "        # z, x, y is the left bottom corner of the patch in the padded image (index shift with pad size)     \n",
    "        # take a patch, with the random point at the center in the padded img\n",
    "        patch  = padded_img[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "        target = padded_lbl[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 188, 188, 1)\n",
      "(188, 188, 188, 1)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"799.9999826604674\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (188, 188, 188)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size, fromLiver=True)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "                   overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 2)) # one-hot encoding with 2 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            # get a patch with corresponding labels from the patch extractor\n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   \n",
    "            \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2    \n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze() \n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=2)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding)(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=True):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "\n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(188, 188, 188, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "    \n",
    "    #TODO: check Conv3DTranspose correctly applied\n",
    "    \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 3 filters, # labels, softmax or ReLU?\n",
    "    finish = Conv3D(2, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             (None, 188, 188, 188, 1)         0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)                                (None, 186, 186, 186, 32)        896               input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization)       (None, 186, 186, 186, 32)        128               conv3d_1[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)                                (None, 184, 184, 184, 64)        55360             batch_normalization_1[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization)       (None, 184, 184, 184, 64)        256               conv3d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)                   (None, 92, 92, 92, 64)           0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)                                (None, 90, 90, 90, 64)           110656            max_pooling3d_1[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization)       (None, 90, 90, 90, 64)           256               conv3d_3[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)                                (None, 88, 88, 88, 128)          221312            batch_normalization_3[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization)       (None, 88, 88, 88, 128)          512               conv3d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)                   (None, 44, 44, 44, 128)          0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)                                (None, 42, 42, 42, 128)          442496            max_pooling3d_2[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization)       (None, 42, 42, 42, 128)          512               conv3d_5[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)                                (None, 40, 40, 40, 256)          884992            batch_normalization_5[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization)       (None, 40, 40, 40, 256)          1024              conv3d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)                   (None, 20, 20, 20, 256)          0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)                                (None, 18, 18, 18, 256)          1769728           max_pooling3d_3[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNormalization)       (None, 18, 18, 18, 256)          1024              conv3d_7[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)                                (None, 16, 16, 16, 512)          3539456           batch_normalization_7[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNormalization)       (None, 16, 16, 16, 512)          2048              conv3d_8[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_1 (Cropping3D)                        (None, 32, 32, 32, 256)          0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTranspose)             (None, 32, 32, 32, 512)          2097664           batch_normalization_8[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)                      (None, 32, 32, 32, 768)          0                 cropping3d_1[0][0]                                \n",
      "                                                                                                    conv3d_transpose_1[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)                                (None, 30, 30, 30, 256)          5308672           concatenate_1[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNormalization)       (None, 30, 30, 30, 256)          1024              conv3d_9[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)                               (None, 28, 28, 28, 256)          1769728           batch_normalization_9[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNormalization)      (None, 28, 28, 28, 256)          1024              conv3d_10[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_2 (Cropping3D)                        (None, 56, 56, 56, 128)          0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTranspose)             (None, 56, 56, 56, 256)          524544            batch_normalization_10[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)                      (None, 56, 56, 56, 384)          0                 cropping3d_2[0][0]                                \n",
      "                                                                                                    conv3d_transpose_2[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)                               (None, 54, 54, 54, 128)          1327232           concatenate_2[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNormalization)      (None, 54, 54, 54, 128)          512               conv3d_11[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)                               (None, 52, 52, 52, 128)          442496            batch_normalization_11[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNormalization)      (None, 52, 52, 52, 128)          512               conv3d_12[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_3 (Cropping3D)                        (None, 104, 104, 104, 64)        0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTranspose)             (None, 104, 104, 104, 128)       131200            batch_normalization_12[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)                      (None, 104, 104, 104, 192)       0                 cropping3d_3[0][0]                                \n",
      "                                                                                                    conv3d_transpose_3[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)                               (None, 102, 102, 102, 64)        331840            concatenate_3[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNormalization)      (None, 102, 102, 102, 64)        256               conv3d_13[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)                               (None, 100, 100, 100, 64)        110656            batch_normalization_13[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNormalization)      (None, 100, 100, 100, 64)        256               conv3d_14[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)                               (None, 100, 100, 100, 2)         130               batch_normalization_14[0][0]                      \n",
      "======================================================================================================================================================\n",
      "Total params: 19,078,402\n",
      "Trainable params: 19,073,730\n",
      "Non-trainable params: 4,672\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch, d, h, w, channels)\n",
      "xdata has shape: (1, 188, 188, 188, 1)\n",
      "ydata has shape: (1, 100, 100, 100, 2)\n",
      "Occuring values in true labels: [0. 1.]\n",
      "Min of input: 0.0\n",
      "Max of input: 0.7025898694992065\n"
     ]
    }
   ],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (188, 188, 188)  # isotropic patch size\n",
    "target_size = (100, 100, 100)  # output size, smaller since valid convolutions are used\n",
    "batch_size  = 1                # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size, fromLiver=True)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a logger which saves the losses and saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    # logg losses and accs, add dice later\n",
    "    def __init__(self, data_dir, model_name):  \n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')        \n",
    "        self.tr_losses = []  \n",
    "#         self.tr_accs = []\n",
    "        self.val_losses = []  \n",
    "#         self.val_accs = []     \n",
    "        self.best_val_loss = float(\"inf\") \n",
    "        self.best_model = None     \n",
    "       \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # add validation info\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.val_accs.append(logs.get('val_acc')) \n",
    "        self.tr_losses.append(logs.get('loss'))\n",
    "#         self.tr_accs.append(logs.get('acc')) \n",
    "        self.plot()\n",
    "\n",
    "        # safe best model after epoch end\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('Best model saved as {}'.format(self.model_filename))\n",
    "         \n",
    "    def plot(self): \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.ylim([0, 1])\n",
    "        n = len(self.val_losses) + 1         \n",
    "        plt.plot(range(1, n), self.tr_losses, label='train loss')         \n",
    "#         plt.plot(range(1, n), self.tr_accs, label='train accuracy')\n",
    "        plt.plot(range(1, n), self.val_losses, label='val loss')        \n",
    "#         plt.plot(range(1, n), self.val_accs, label='val accuracy') \n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home4/mbotros/ISMI_project/Task03_Liver\n"
     ]
    }
   ],
   "source": [
    "# make a data dir to store best model\n",
    "print(os.getcwd())\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to inline, I don't know how to plot in notebook mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "Dice loss seems to be a good pick for 3D segmentation with class inbalances. We have to look if this works like this.\n",
    "In our case y_pred is output of the softmax.(see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "\n",
    "$$ \\textbf{Dice loss} =  1 - \\frac{2 \\hspace{0.3em}|X \\cap Y|}{|X|+ |Y|} $$\n",
    "\n",
    "\n",
    "**For binary volumes of N voxels (Milletari et al., 2016, VNet):**\n",
    "\n",
    "$$ \\textbf{Dice loss} = 1 -\\frac{2 \\sum_{i}^{N} p_{i} g_{i}}{\\sum_{i}^{N} p_{i}^{2}+\\sum_{i}^{N} g_{i}^{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss as above\n",
    "def dice_loss_bv(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    G = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    return 1. - (2. * K.sum(P * G) + epsilon) / (K.sum(P**2.) + K.sum(G**2.) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed in Milletari et al. [8] as a loss function, the 2-class variant of the Dice loss, denoted DL2, can be expressed as**\n",
    "\n",
    "$$\n",
    "\\mathrm{DL}_{2}=1-\\frac{\\sum_{n=1}^{N} p_{n} r_{n}+\\epsilon}{\\sum_{n=1}^{N} p_{n}+r_{n}+\\epsilon}-\\frac{\\sum_{n=1}^{N}\\left(1-p_{n}\\right)\\left(1-r_{n}\\right)+\\epsilon}{\\sum_{n=1}^{N} 2-p_{n}-r_{n}+\\epsilon}\n",
    "$$\n",
    "\n",
    "Sudre, Carole H., et al. \"Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.\" https://arxiv.org/pdf/1707.03237.pdf\n",
    "\n",
    "\n",
    "**Let R be the reference foreground segmentation\n",
    "(gold standard) with voxel values $r_n$, and P the predicted probabilistic map for the foreground label over N image elements $p_n$, with the background class probability being 1 − P. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation in a binary classification (foreground vs. background) formulation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    R = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    a = K.sum(P * R) + epsilon\n",
    "    b = K.sum(P + R) + epsilon\n",
    "    c = K.sum((1 - P) * (1 - R)) + epsilon \n",
    "    d = K.sum((2 - P - R)) + epsilon\n",
    "    \n",
    "    return 1 - a/b - c/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 10**-4\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "metrics         = ['accuracy']  # probably not useful (?)\n",
    "steps_per_epoch = 5\n",
    "epochs          = 10\n",
    "logger          = Logger(data_dir, '3D-UNet-21-05')\n",
    "\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# compile model\n",
    "unet_3d.compile(optimizer=optimizer, loss=dice_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,64,184,184,184]\n\t [[Node: max_pooling3d_1/MaxPool3D = MaxPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_2/cond/Merge)]]\n\t [[Node: loss/mul/_983 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8852_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'max_pooling3d_1/MaxPool3D', defined at:\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-26afc44b8a32>\", line 1, in <module>\n    unet_3d = build_unet_3d(initial_filters=32, padding='valid')\n  File \"<ipython-input-15-2f92b20ad753>\", line 11, in build_unet_3d\n    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 374, in call\n    data_format=self.data_format)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 432, in _pooling_function\n    padding, data_format, pool_mode='max')\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 4024, in pool3d\n    data_format=tf_data_format)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2870, in max_pool3d\n    padding=padding, data_format=data_format, name=name)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,64,184,184,184]\n\t [[Node: max_pooling3d_1/MaxPool3D = MaxPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_2/cond/Merge)]]\n\t [[Node: loss/mul/_983 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8852_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,64,184,184,184]\n\t [[Node: max_pooling3d_1/MaxPool3D = MaxPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_2/cond/Merge)]]\n\t [[Node: loss/mul/_983 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8852_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-90c0cb04c1fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=[logger])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,64,184,184,184]\n\t [[Node: max_pooling3d_1/MaxPool3D = MaxPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_2/cond/Merge)]]\n\t [[Node: loss/mul/_983 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8852_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'max_pooling3d_1/MaxPool3D', defined at:\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-26afc44b8a32>\", line 1, in <module>\n    unet_3d = build_unet_3d(initial_filters=32, padding='valid')\n  File \"<ipython-input-15-2f92b20ad753>\", line 11, in build_unet_3d\n    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 374, in call\n    data_format=self.data_format)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 432, in _pooling_function\n    padding, data_format, pool_mode='max')\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 4024, in pool3d\n    data_format=tf_data_format)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2870, in max_pool3d\n    padding=padding, data_format=data_format, name=name)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/mbotros/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,64,184,184,184]\n\t [[Node: max_pooling3d_1/MaxPool3D = MaxPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_2/cond/Merge)]]\n\t [[Node: loss/mul/_983 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8852_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "unet_3d.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=image_generator_val,\n",
    "                    verbose=1,\n",
    "                    validation_steps=1,\n",
    "                    callbacks=[logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(image, patch_size, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    image       : the input image (as numpy)\n",
    "    patch_size  : patch_size of the input for the UNet\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    z, y, x = patch_size\n",
    "    \n",
    "    # pad with min value from image, always safe\n",
    "    min_val = np.min(image)\n",
    "    \n",
    "    # size of padding for each dimension\n",
    "    pad_z = (z - target_size[0]) // 2\n",
    "    pad_x = (x - target_size[1]) // 2\n",
    "    pad_y = (y - target_size[2]) // 2\n",
    "    \n",
    "    # pad with a tuple for how much on each side for every dimension\n",
    "    padded_input = np.pad(image, ((pad_z, pad_z), (pad_x, pad_x), (pad_y, pad_y)), 'constant', constant_values=min_val)\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    image       : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "    image = np.clip(image, -1000, 3000)\n",
    "    image = (image - np.min(image)) / np.ptp(image)    \n",
    "    \n",
    "    # pad the input image:\n",
    "    pad_img = padding(image, patch_size, target_size)  \n",
    "\n",
    "    print(\"Image size: {}\".format(image.shape))\n",
    "    print(\"Padded image size: {}\".format(pad_img.shape))\n",
    "    \n",
    "    dims = image.shape\n",
    "    # how many times target size fits in a dimension \n",
    "    pz = dims[0] // target_size[0] \n",
    "    px = dims[1] // target_size[1] \n",
    "    py = dims[2] // target_size[2] \n",
    "    \n",
    "    # segmentation map, same size as input image\n",
    "    segmentation = np.zeros(image.shape)   \n",
    "    \n",
    "    for z in range(pz, -1, -1):         \n",
    "        for x in range(px, -1, -1):\n",
    "            for y in range(py, -1, -1):  \n",
    "                \n",
    "                # shift starting point with target_size\n",
    "                start_z = z * target_size[0]\n",
    "                start_x = x * target_size[1]\n",
    "                start_y = y * target_size[2]\n",
    "                \n",
    "                # if the patch does not fit:\n",
    "                if start_z + patch_size[0] > pad_img.shape[0]:\n",
    "                    start_z = pad_img.shape[0] - patch_size[0]\n",
    "                if start_x + patch_size[1] > pad_img.shape[1]:\n",
    "                    start_x = pad_img.shape[1] - patch_size[1]\n",
    "                if start_y + patch_size[2] > pad_img.shape[2]:\n",
    "                    start_y = pad_img.shape[2] - patch_size[2]\n",
    "                \n",
    "                # Get patch: shift with target_size, take patch_size                \n",
    "                patch = pad_img[start_z:start_z + patch_size[0], \n",
    "                                start_x:start_x + patch_size[1], \n",
    "                                start_y:start_y + patch_size[2]]     \n",
    "\n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = model.predict(patch)\n",
    "\n",
    "                # Put the prediction in segmentation map, shift with target_size, take target_size\n",
    "                segmentation[start_z:start_z + target_size[0], \n",
    "                             start_x:start_x + target_size[1], \n",
    "                             start_y:start_y + target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "    \n",
    "    return segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an image and a label from the validation set\n",
    "image = val_set.imgs[2][200:400, :, -200:]\n",
    "label = val_set.lbls[2][200:400, :, -200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model, note that we shuffle now and you shouldnt use the old models trained with unshuffeled data\n",
    "unet_3d = load_model(os.path.join(data_dir, '3D_UNet_DL2' + '.h5'), custom_objects={'dice_loss': dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the segmentation map\n",
    "segmentation = predict_image_segmentation(unet_3d, image, target_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(segmentation, return_counts=True))\n",
    "print(np.unique(label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot slices\n",
    "%matplotlib notebook\n",
    "s = 100\n",
    "slice_img = image[s, :, :]\n",
    "slice_lbl = label[s, :, :]\n",
    "slice_seg = segmentation[s, :, :]\n",
    "\n",
    "masked_lbl = np.ma.masked_where(slice_lbl < 1, slice_lbl)\n",
    "masked_seg = np.ma.masked_where(slice_seg < 1, slice_seg)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1).set_title('Prediction')\n",
    "plt.imshow(slice_img, cmap='gray')\n",
    "plt.imshow(masked_seg, cmap='coolwarm', alpha = 0.75)\n",
    "\n",
    "plt.subplot(1,2,2).set_title('Ground truth')\n",
    "plt.imshow(slice_img, cmap='gray')\n",
    "plt.imshow(masked_lbl, cmap='coolwarm', alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show segmentation\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=segmentation, overlay_1_thres=1, \n",
    "                   overlay_2=segmentation, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=label, overlay_1_thres=1, \n",
    "                   overlay_2=label, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
