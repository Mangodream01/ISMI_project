{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D UNet for segmentation of the liver and spleen\n",
    "\n",
    "Inspiration for improvements: **nnU-Net: Self-adapting Framework\n",
    "for U-Net-Based Medical Image Segmentation** (https://arxiv.org/pdf/1809.10486.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import label as scipy_label\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "from IPython.display import clear_output\n",
    "import pickle \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "import h5py\n",
    "import ipywidgets\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we use gpu or cpu\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "assert len(K.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task09_Spleen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)   \n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images without label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to train files\n",
    "train_imgs = [train_instance['image'] for train_instance in train_paths]\n",
    "train_lbls = [train_instance['label'] for train_instance in train_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, y, x)\n",
    "\n",
    "Resample to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load one image and label at a time\n",
    "def resample_and_save(paths, order, filename):     \n",
    "    print('Storing resampled dataset at {}.'.format(os.path.join(os.getcwd(), filename)))    \n",
    "    \n",
    "    # make a h5py file to store images as numpy in\n",
    "    with h5py.File(filename, 'w') as f: \n",
    "        for i, path in enumerate(tqdm(paths)):            \n",
    "            \n",
    "            # read the image\n",
    "            img = sitk.ReadImage(path)\n",
    "            \n",
    "            # get the spacing\n",
    "            spacing = img.GetSpacing()\n",
    "            \n",
    "            # change order\n",
    "            x, y, z = spacing           \n",
    "            spacing = (z, y, x)      # swap x and z\n",
    "\n",
    "            # convert to numpy\n",
    "            np_img = sitk.GetArrayFromImage(img)\n",
    "\n",
    "            # apply zoom\n",
    "            np_img_re = zoom(np_img, spacing, order=order)\n",
    "            \n",
    "            print(\"Index: {}, before: {}, spacing: {}, after: {}\".format(i, np_img.shape, spacing, np_img_re.shape))\n",
    "\n",
    "            # save the resamples img/label\n",
    "            dataset = f.create_dataset(str(i), data=np_img_re)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use order 3 interpolation for imgs, order 0 for labels (neigherest neighbour interpolation)\n",
    "# resample_and_save(train_imgs, order=3, filename='spleen_resampled_train_imgs.h5py')\n",
    "# resample_and_save(train_lbls, order=0, filename='spleen_resampled_train_lbls.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the  train data\n",
    "Start here if you saved the resampled images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir\n",
    "data_path = './Task09_Spleen/'\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk, note that the keys are not in order, for the test set load in order\n",
    "np_train_imgs = []\n",
    "np_train_lbls = []\n",
    "\n",
    "with h5py.File('spleen_resampled_train_imgs.h5py', 'r') as f: \n",
    "    for img in tqdm(f):\n",
    "        dset = f[img]\n",
    "        np_train_imgs.append(dset[:])\n",
    "    \n",
    "with h5py.File('spleen_resampled_train_lbls.h5py', 'r') as f: \n",
    "    for lbl in tqdm(f):\n",
    "        dset = f[lbl]     \n",
    "        np_train_lbls.append(dset[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of the labels\n",
    "This might be useful for later if we need class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the labels of train images\n",
    "sums = np.zeros(3)\n",
    "for lbs in tqdm(np_train_lbls):\n",
    "    labels, counts = np.unique(lbs, return_counts=True)\n",
    "    \n",
    "    # if there are only 2 labels\n",
    "    if len(counts) == 2:\n",
    "        sums[:2]+=counts\n",
    "    else:\n",
    "        sums+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentages of voxels.\n",
    "total = sum(sums)\n",
    "print(\"{:.2f}% background, {:.2f}% spleen, {:.2f}% cancer.\".format(sums[0]/total*100, sums[1]/total*100, sums[2]/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation\n",
    "During training we want to select the best model, which is done with the validation set (unseen data for the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training set and validation set\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(np_train_imgs))\n",
    "\n",
    "val_set   = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "train_set = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])\n",
    "\n",
    "print('{} images in val set'.format(val_set.get_lenght()))\n",
    "print('{} images in train set'.format(train_set.get_lenght()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, fromSpleen):\n",
    "        self.patch_size = patch_size \n",
    "        self.fromSpleen = fromSpleen\n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # size of patch in each dimension\n",
    "        pz, px, py = self.patch_size\n",
    "        \n",
    "#         print('Patch_size: {}'.format(patch_size))\n",
    "#         print('Image_size: {}'.format(image.shape))\n",
    "\n",
    "        # pad with the min value in the image\n",
    "        min_val = np.min(image)\n",
    "        \n",
    "        # pad with half the patch size, I assume even patch size\n",
    "        padded_img = np.pad(image, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant', constant_values=min_val)\n",
    "        padded_lbl = np.pad(label, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant')\n",
    "        \n",
    "#         print('Padded_size: {}'.format(padded_img.shape))\n",
    "\n",
    "        # random between [0., 1.), if fromSpleen is true takes 75% of patches from spleen\n",
    "        p = random.random()\n",
    "\n",
    "        # centre of the patch: a random point from the spleen in the non padded image\n",
    "        if self.fromSpleen and p > 0.25:\n",
    "            \n",
    "            # getting the spleen labeled points\n",
    "            liver_ind = np.argwhere(label == 1)  \n",
    "            \n",
    "            # get a random point from the spleen labeled points\n",
    "            r = randint(0, len(liver_ind))\n",
    "            z = liver_ind[r][0]\n",
    "            x = liver_ind[r][1]\n",
    "            y = liver_ind[r][2]\n",
    "            \n",
    "        # centre of the patch: a random location in the non padded image    \n",
    "        else:\n",
    "            dims = image.shape\n",
    "            z = randint(0, dims[0]) \n",
    "            x = randint(0, dims[1]) \n",
    "            y = randint(0, dims[2])   \n",
    "            \n",
    "        # z, x, y is the left bottom corner of the patch in the padded image (index shift with pad size)     \n",
    "        # take a patch, with the random point at the center in the padded img\n",
    "        patch  = padded_img[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "        target = padded_lbl[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (156, 156, 156)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size, fromSpleen=True)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "                   overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 2)) # one-hot encoding with 2 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            # get a patch with corresponding labels from the patch extractor\n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   \n",
    "            \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2    \n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze()\n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=2)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding, kernel_initializer='he_normal')(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding, kernel_initializer='he_normal')(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding, kernel_initializer='he_normal')(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=True, print_model=False):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "    \n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(156, 156, 156, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "        \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 2 filters, #labels, softmax\n",
    "    finish = Conv3D(2, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    \n",
    "    if print_model:\n",
    "        print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid', batchnorm=False, print_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (156, 156, 156)  # isotropic patch size\n",
    "target_size = (68, 68, 68)     # output size, smaller since valid convolutions are used\n",
    "batch_size  = 2                # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size, fromSpleen=True)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))\n",
    "\n",
    "# show the patches, to check the batchcreator, mid slice\n",
    "x = patch_size[0] // 2\n",
    "patch = x_data[0, x, :, :].squeeze()\n",
    "\n",
    "# get labels from one hot\n",
    "label = np.argmax(y_data[0, :, :, :], axis=-1)\n",
    "\n",
    "# labels were cropped, so pad them\n",
    "p = (patch_size[0] - target_size[0]) // 2\n",
    "padded_lbl = np.pad(label, ((p, p), (p, p), (p, p)), 'constant', constant_values=3.)\n",
    "\n",
    "# makes masks to plot label overlay\n",
    "masked_crop = np.ma.masked_where(padded_lbl != 3, padded_lbl)\n",
    "masked_lbl  = np.ma.masked_where(padded_lbl == 0, padded_lbl)\n",
    "\n",
    "# take the mid slice aswell\n",
    "masked_lbl  = masked_lbl[x, :, :]            \n",
    "masked_crop = masked_crop[x, :, :]\n",
    "\n",
    "# show x_data (no labels) and y_data, which are the labels for the center crop of x_data\n",
    "# y_data is plotted with padding to check\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1, title='x_data')\n",
    "plt.imshow(patch, cmap='gray')\n",
    "plt.subplot(1, 2, 2, title='y_data')\n",
    "plt.imshow(patch, cmap='gray')\n",
    "plt.imshow(masked_lbl, cmap='coolwarm', alpha = 0.75)\n",
    "plt.imshow(masked_crop, cmap='gray', alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a logger which saves the losses and saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    # logg losses, removed accs for now\n",
    "    def __init__(self, data_dir, model_name, base_model):  \n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')        \n",
    "        self.tr_losses = []  \n",
    "        self.val_losses = []      \n",
    "        self.best_val_loss = float(\"inf\")   \n",
    "        self.base_model = base_model\n",
    "       \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # add validation info\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.tr_losses.append(logs.get('loss'))\n",
    "        self.plot()\n",
    "\n",
    "        # safe best model after epoch end\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            \n",
    "            # call save on the base model instead of the parallel model, both models share the same weights\n",
    "            self.base_model.save(self.model_filename) # save best model to disk\n",
    "            print('Best model saved as {}'.format(self.model_filename))\n",
    "         \n",
    "    def plot(self): \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        n = len(self.val_losses) + 1         \n",
    "        plt.plot(range(1, n), self.tr_losses, label='train loss')         \n",
    "        plt.plot(range(1, n), self.val_losses, label='val loss')        \n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data dir to store best model\n",
    "print(os.getcwd())\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Loss \n",
    "------------------------------------------------\n",
    "Dice loss seems to be a good pick for 3D segmentation with class inbalances. (see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "\n",
    "**Note that these are attempts for binary segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{DL}_{2}=1-\\frac{\\sum_{n=1}^{N} p_{n} r_{n}+\\epsilon}{\\sum_{n=1}^{N} p_{n}+r_{n}+\\epsilon}-\\frac{\\sum_{n=1}^{N}\\left(1-p_{n}\\right)\\left(1-r_{n}\\right)+\\epsilon}{\\sum_{n=1}^{N} 2-p_{n}-r_{n}+\\epsilon}\n",
    "$$\n",
    "\n",
    "Let R be the reference foreground segmentation (gold standard) with voxel values $r_n$, and P the predicted probabilistic map for the foreground label over N image elements $p_n$, with the background class probability being 1 − P. \n",
    "\n",
    "Sudre, Carole H., et al. \"Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.\" https://arxiv.org/pdf/1707.03237.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_2(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation in a binary classification (foreground vs. background) formulation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * K.constant([0., 1.]), axis=-1)   # shape = (b, z, h, w)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    R = K.sum(y_true * K.constant([0., 1.]), axis=-1)   # shape = (b, z, h, w)\n",
    "    \n",
    "    a = K.sum(P * R, axis=(1, 2, 3)) \n",
    "    b = K.sum(P + R, axis=(1, 2, 3)) + epsilon\n",
    "    c = K.sum((1. - P) * (1. - R), axis=(1, 2, 3)) \n",
    "    d = K.sum((2. - P - R), axis=(1, 2, 3)) + epsilon\n",
    "    \n",
    "    return 1 - a/b - c/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{DL}_3 =  1 - \\frac{2 \\hspace{0.3em}|X \\cap Y|}{|X|+ |Y|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_3(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    This seems correct to me.\n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver)\n",
    "    P = K.sum(y_pred * K.constant([0., 1.]), axis=-1)   # shape = (b, d, h, w)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    R = K.sum(y_true * K.constant([0., 1.]), axis=-1)   # shape = (b, d, h, w)\n",
    "    \n",
    "    #  |X| ∩ |Y|, true positives\n",
    "    true_positives = K.sum(P * R, axis=(1, 2, 3))\n",
    "    \n",
    "    # this is |Y| number of voxels predicted to be fg (probabilities) \n",
    "    fg_voxels_p = K.sum(P, axis=(1, 2, 3))\n",
    "    \n",
    "    # this is |X|, should be the number of foreground voxels in the ground truth\n",
    "    fg_voxels_r = K.sum(R, axis=(1, 2, 3))\n",
    "    \n",
    "    return 1. - (2. * true_positives + epsilon) / (fg_voxels_r + fg_voxels_p + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this loss function\n",
    "loss_function = dice_loss_3\n",
    "\n",
    "# all zeros and all ones in one hot        \n",
    "all_bg = np.ones(shape=(2, 68, 68, 68, 2)) * [1, 0]\n",
    "all_fg = np.ones(shape=(2, 68, 68, 68, 2)) * [0, 1]\n",
    "\n",
    "# some fg in patch 1\n",
    "some_fg  = np.copy(all_bg)\n",
    "some_fg2 = np.copy(all_bg)\n",
    "some_fg[0, :34, :34, :34]  = [0, 1]\n",
    "some_fg2[0, :17, :17, :17] = [0, 1]\n",
    "\n",
    "print('A bit of fg, predicts exactly that bit of fg:{}'.format(loss_function(some_fg, some_fg).eval(session=sess)))\n",
    "print('A bit of fg, predicts exactly half of that bit of fg:{}'.format(loss_function(some_fg, some_fg2).eval(session=sess)))\n",
    "\n",
    "print('All is bg, predicts all fg: {}'.format(loss_function(all_bg, all_fg).eval(session=sess)))\n",
    "print('All is fg, predicts all bg: {}'.format(loss_function(all_fg, all_bg).eval(session=sess)))\n",
    "\n",
    "print('Complete overlap in both patches fg: {}'.format(loss_function(all_fg, all_fg).eval(session=sess)))\n",
    "print('Complete overlap in both patches bg: {}'.format(loss_function(all_bg, all_bg).eval(session=sess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing class weights\n",
    "For training with a weighted Cross Entropy loss we need class weights. Frequent classes will contribute less to loss.\n",
    "Instead of computing class weights from the whole train set, we might want to compute them from the batch in which we sample patches from the liver. This changes the foreground/background ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we want class weight based on the fg/bg ratio in the patches\n",
    "def compute_weights_from_batch(batch_gen, batch_size):\n",
    "    '''\n",
    "    Compute class weights for a given batch generator. \n",
    "    The weight for a class is computed as the inverse of its volume.\n",
    "    batch_gen : the generator from which to get a batch\n",
    "    batch_size: size of the batch the generator has to make\n",
    "    '''\n",
    "    # create a batch of batch_size\n",
    "    x_data, y_data = batch_gen.create_image_batch(batch_size=batch_size)    \n",
    "    labels = np.argmax(y_data, axis=-1)    \n",
    "    l, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    print(l)\n",
    "    print(counts) \n",
    "    \n",
    "    # inverse of volume\n",
    "    return 1 / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_from_batch = compute_weights_from_batch(batch_generator_train, batch_size=100)\n",
    "print('Class weights from batch: {}'.format(weights_from_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network \n",
    "Inference is simpler with a batch size of 1, so we train with a parallel model which requires an even batch size and is faster, but save the weights from the base model. This way we can use it for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid', batchnorm=False, print_model=False)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 3*10**-4\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "steps_per_epoch = 200\n",
    "epochs          = 100\n",
    "batch_size      = 2\n",
    "logger          = Logger(data_dir, 'spleen-14-06', base_model=unet_3d)\n",
    "\n",
    "# get generators\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# make parallel model for using two gpus\n",
    "parallel_model = multi_gpu_model(unet_3d, gpus=2)                   # batchsize should be a multiple of #gpu's\n",
    "\n",
    "# compile model\n",
    "parallel_model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "parallel_model.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=image_generator_val, \n",
    "                    class_weight=weights_from_batch,\n",
    "                    verbose=1,\n",
    "                    validation_steps=25,\n",
    "                    callbacks=[logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(image, patch_size, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    image       : the input image (as numpy)\n",
    "    patch_size  : patch_size of the input for the UNet\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    z, y, x = patch_size\n",
    "    \n",
    "    # pad with min value from image, always safe\n",
    "    min_val = np.min(image)\n",
    "    \n",
    "    # size of padding for each dimension\n",
    "    pad_z = (z - target_size[0]) // 2\n",
    "    pad_x = (x - target_size[1]) // 2\n",
    "    pad_y = (y - target_size[2]) // 2\n",
    "    \n",
    "    # pad with a tuple for how much on each side for every dimension\n",
    "    padded_input = np.pad(image, ((pad_z, pad_z), (pad_x, pad_x), (pad_y, pad_y)), 'constant', constant_values=min_val)\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    image       : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "    image = np.clip(image, -1000, 3000)\n",
    "    image = (image - np.min(image)) / np.ptp(image)    \n",
    "    \n",
    "    # pad the input image:\n",
    "    pad_img = padding(image, patch_size, target_size)  \n",
    "\n",
    "#     print(\"Image size: {}\".format(image.shape))\n",
    "#     print(\"Padded image size: {}\".format(pad_img.shape))\n",
    "    \n",
    "    dims = image.shape\n",
    "    # how many times target size fits in a dimension \n",
    "    pz = dims[0] // target_size[0] \n",
    "    px = dims[1] // target_size[1] \n",
    "    py = dims[2] // target_size[2] \n",
    "    \n",
    "    # segmentation map, same size as input image\n",
    "    segmentation = np.zeros(image.shape)   \n",
    "    \n",
    "    for z in range(pz, -1, -1):         \n",
    "        for x in range(px, -1, -1):\n",
    "            for y in range(py, -1, -1):  \n",
    "                \n",
    "                # shift starting point with target_size\n",
    "                start_z = z * target_size[0]\n",
    "                start_x = x * target_size[1]\n",
    "                start_y = y * target_size[2]\n",
    "                \n",
    "                # if the patch does not fit:\n",
    "                if start_z + patch_size[0] > pad_img.shape[0]:\n",
    "                    start_z = pad_img.shape[0] - patch_size[0]\n",
    "                if start_x + patch_size[1] > pad_img.shape[1]:\n",
    "                    start_x = pad_img.shape[1] - patch_size[1]\n",
    "                if start_y + patch_size[2] > pad_img.shape[2]:\n",
    "                    start_y = pad_img.shape[2] - patch_size[2]\n",
    "                \n",
    "                # Get patch: shift with target_size, take patch_size                \n",
    "                patch = pad_img[start_z:start_z + patch_size[0], \n",
    "                                start_x:start_x + patch_size[1], \n",
    "                                start_y:start_y + patch_size[2]]     \n",
    "\n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = model.predict(patch)\n",
    "\n",
    "                # Put the prediction in segmentation map, shift with target_size, take target_size\n",
    "                segmentation[start_z:start_z + target_size[0], \n",
    "                             start_x:start_x + target_size[1], \n",
    "                             start_y:start_y + target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "    \n",
    "    return segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: connected component labeling and resampling\n",
    "We use the connected component labeling algorithm to create groups of connected segmentations. We then choose the largest group as our final foreground segmentation and segment the rest as background. Afterwards, all images will be resampled back to its original dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_components(segmentation):\n",
    "    \"\"\"\n",
    "    Function to perform connected component labeling and remove everything except for the largest foreground segmentation. \n",
    "    segmentation: full segmentation of an image before post-processing.\n",
    "    \"\"\"    \n",
    "    # Find objects > 0.5 (= segmented as liver)\n",
    "    segmentation_new, nr_objects = scipy_label(segmentation > 0.5) \n",
    "#     print(\"Number of objects is \", nr_objects)\n",
    "\n",
    "    # Only keep the largest object\n",
    "    unique, counts = np.unique(segmentation_new, return_counts=True)\n",
    "    largest_object = np.argmax(counts[1:]) + 1\n",
    "#     print(\"Largest object is \", largest_object)\n",
    "    segmentation_new[segmentation_new != largest_object] = 0\n",
    "    segmentation_new[segmentation_new == largest_object] = 1\n",
    "    return segmentation_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the prediction\n",
    "To test the current model, we can inspect segmenations it makes for validation images to see whats going wrong. We can assess the quality of the segmentation with a Dice score:\n",
    "\n",
    "$$D S C=\\frac{2 T P}{2 T P+F P+F N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsc(y_true, y_pred):\n",
    "    '''\n",
    "    Computes the dice similarity score.\n",
    "    y_true: Reference standard\n",
    "    y_pred: Predicted segmentation\n",
    "    '''\n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y_true == 0))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n",
    "    \n",
    "    return 2. * TP / (2. * TP  + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(y_true, y_pred, slice_nr):\n",
    "    \n",
    "    # locations of tp, tn, fp, fn\n",
    "    TP = np.logical_and(y_pred == 1, y_true == 1) \n",
    "    TN = np.logical_and(y_pred == 0, y_true == 0) \n",
    "    FP = np.logical_and(y_pred == 1, y_true == 0) \n",
    "    FN = np.logical_and(y_pred == 0, y_true == 1) \n",
    "    \n",
    "    # compute masks\n",
    "    mask_tp = np.ma.masked_where(TP == 0, TP)\n",
    "    mask_tn = np.ma.masked_where(TN == 0, TN)\n",
    "    mask_fp = np.ma.masked_where(FP == 0, FP)\n",
    "    mask_fn = np.ma.masked_where(FN == 0, FN)\n",
    "\n",
    "    # plot masks, rotate for the doctor\n",
    "    plt.figure()\n",
    "    #plt.title('Location of the errors.')\n",
    "    plt.imshow(np.rot90(mask_tp[slice_nr, :, :], 2), cmap='summer')\n",
    "    plt.imshow(np.rot90(mask_tn[slice_nr, :, :], 2), cmap='gray')\n",
    "    plt.imshow(np.rot90(mask_fp[slice_nr, :, :], 2), cmap='RdYlGn')\n",
    "    plt.imshow(np.rot90(mask_fn[slice_nr, :, :], 2), cmap='coolwarm')\n",
    "    \n",
    "    # get cmaps for legend\n",
    "    summer = matplotlib.cm.get_cmap('summer')\n",
    "    gray = matplotlib.cm.get_cmap('gray')\n",
    "    RdYlGn = matplotlib.cm.get_cmap('RdYlGn')\n",
    "    coolwarm = matplotlib.cm.get_cmap('coolwarm')\n",
    "    \n",
    "    # plot legend\n",
    "    plt.text(460, 40, 'TP', bbox={'facecolor': summer(0), 'pad': 10})\n",
    "    plt.text(460, 100, 'TN', color='white', bbox={'facecolor': gray(0), 'pad': 10})\n",
    "    plt.text(460, 160, 'FP', bbox={'facecolor': RdYlGn(0), 'pad': 10})\n",
    "    plt.text(460, 220, 'FN', bbox={'facecolor': coolwarm(0), 'pad': 10})\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # save figure\n",
    "    plt.savefig('image_14_errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the validation set\n",
    "# declare data dir and patch/target size (needed to make a prediction as well)\n",
    "data_dir = '../data'\n",
    "patch_size  = (156, 156, 156)  # isotropic patch size\n",
    "target_size = (68, 68, 68)     # output size, smaller since valid convolutions are used\n",
    "\n",
    "# load best model\n",
    "unet_3d = load_model(os.path.join(data_dir, 'spleen-14-06' + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment every image in the validation set\n",
    "val_set_segs = []\n",
    "\n",
    "for image in tqdm(val_set.imgs):\n",
    "    seg_map = predict_image_segmentation(unet_3d, image, target_size, patch_size)\n",
    "    val_set_segs.append(seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply connected components\n",
    "val_set_segs = [connected_components(seg) for seg in val_set_segs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the segmentations to disk, because segmentation of images takes a while and we might want to check it later..\n",
    "with h5py.File('val_set_segmentations_spleen-14-06.h5py', 'w') as f: \n",
    "    for i, seg in enumerate(tqdm(val_set_segs)):\n",
    "        f.create_dataset(str(i), data=seg)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segmentations\n",
    "val_set_segs = []\n",
    "\n",
    "with h5py.File('val_set_segmentations.h5py', 'r') as f:     \n",
    "    for i in tqdm(range(len(val_set.lbls))):\n",
    "        dset = f[str(i)]     \n",
    "        val_set_segs.append(dset[:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dice scores for every image of the validation set\n",
    "dice_scores_val = []\n",
    "for i in tqdm(range(val_set.get_lenght())):\n",
    "    d = dsc(val_set.lbls[i], val_set_segs[i])\n",
    "    print(d)\n",
    "    dice_scores_val.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean dice validation set: {}'.format(np.mean(dice_scores_val)))\n",
    "print('Lowest scoring image: {}'.format(np.argmin(dice_scores_val)))\n",
    "print('Dice of the lowest scoring image: {}'.format(np.min(dice_scores_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot slices\n",
    "def compare_with_groundtruth(image, label, segmentation, slice_nr=None):\n",
    "    \n",
    "    # mid slice\n",
    "    s = image.shape[0] // 2\n",
    "    \n",
    "    # or a given slice \n",
    "    if slice_nr:\n",
    "        s = slice_nr\n",
    "        \n",
    "    slice_img = image[s, :, :]\n",
    "    slice_lbl = label[s, :, :]\n",
    "    slice_seg = segmentation[s, :, :]\n",
    "\n",
    "    # masks for plotting the label overlay, rotate for the doctor\n",
    "    masked_lbl = np.rot90(np.ma.masked_where(slice_lbl < 1, slice_lbl), 2)\n",
    "    masked_seg = np.rot90(np.ma.masked_where(slice_seg < 1, slice_seg), 2)\n",
    "    \n",
    "    # also rotate the image for the doctor\n",
    "    slice_img = np.rot90(slice_img, 2)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)#.set_title('Prediction')\n",
    "    plt.imshow(slice_img, cmap='gray')\n",
    "    if (np.any(masked_seg)):\n",
    "        plt.imshow(masked_seg, cmap='coolwarm', alpha = 0.75)\n",
    "\n",
    "    plt.subplot(1,2,2)#.set_title('Ground truth')\n",
    "    plt.imshow(slice_img, cmap='gray')\n",
    "    if (np.any(masked_lbl)):\n",
    "        plt.imshow(masked_lbl, cmap='coolwarm', alpha = 0.75)\n",
    "    plt.savefig('image_14.png')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image with labels and predicted segmentation\n",
    "i = 14\n",
    "image        = val_set.imgs[i]\n",
    "lbl          = val_set.lbls[i]\n",
    "segmentation = val_set_segs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with groundtruth\n",
    "compare_with_groundtruth(image, lbl, segmentation, slice_nr=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the errors\n",
    "plot_errors(lbl, segmentation, slice_nr=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on the test set\n",
    "To test our methods we will make predictions for the test set and submit them to the MSD grandchallenge website.\n",
    "- Resample to 1mm x 1mm x 1mm resolution\n",
    "- Load resampled test set\n",
    "- Load model and make segmentations\n",
    "- Do post processing\n",
    "- Resample back to original resolution and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample_and_save(test_paths, order=3, filename='resampled_test_imgs.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the resampled test set, in right order, keys are not in order\n",
    "test_set = []\n",
    "\n",
    "with h5py.File('resampled_test_imgs.h5py', 'r') as f:     \n",
    "    for i in tqdm(range(len(test_paths))):        \n",
    "        dset = f[str(i)]     \n",
    "        test_set.append(dset[:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare data dir and patch/target size (needed to make a prediction as well)\n",
    "data_dir = '../data'\n",
    "patch_size  = (156, 156, 156)  # isotropic patch size\n",
    "target_size = (68, 68, 68)     # output size, smaller since valid convolutions are used\n",
    "\n",
    "# load best model/weights\n",
    "unet_3d = load_model(os.path.join(data_dir, 'spleen-14-06' + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment every image in the test set\n",
    "test_set_seg = []\n",
    "\n",
    "for image in tqdm(test_set):\n",
    "    seg_map = predict_image_segmentation(unet_3d, image, target_size, patch_size)\n",
    "    test_set_seg.append(seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir to store result\n",
    "result_dir = './result/Task09_spleen'\n",
    "if not os.path.exists(result_dir):    \n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg in enumerate(tqdm(test_set_seg)):\n",
    "    # post processing, connected component\n",
    "    final_segmentation = connected_components(seg)    \n",
    "    \n",
    "    # path to file and name of the file\n",
    "    path = test_paths[i]\n",
    "    filename = path[11:]\n",
    "    \n",
    "    # resample back to original spacings/shape\n",
    "    img = sitk.ReadImage(path) \n",
    "    original_size = img.GetSize()\n",
    "    a, b, c = original_size\n",
    "    x, y, z = img.GetSpacing()     \n",
    "    spacing = (1./z, 1./y, 1./x)               # swap x and z, invert spacing\n",
    "    \n",
    "    # apply spacing    \n",
    "    np_resampled   = zoom(final_segmentation, spacing, order=0)\n",
    "    d, e, f = np_resampled.shape\n",
    "    \n",
    "    # make empty array in right size\n",
    "    np_seg = np.zeros((c, b, a))\n",
    "    np_seg[:d, :e, :f] = np_resampled[:c, :b, :a]\n",
    "    sitk_resampled = sitk.GetImageFromArray(np_seg)  \n",
    "    \n",
    "    # check output and original size\n",
    "    output_size   = sitk_resampled.GetSize()\n",
    "#     print('Original size: {}, output_size: {}'.format(original_size, output_size))\n",
    "    assert(output_size == original_size)\n",
    "    \n",
    "    # storage path\n",
    "    storage_path = os.path.join(result_dir, filename)\n",
    "#     print('Stored in: {}'.format(storage_path))\n",
    "    \n",
    "    # save segmentation map\n",
    "    sitk.WriteImage(sitk.Cast(sitk_resampled, sitk.sitkUInt8), storage_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
