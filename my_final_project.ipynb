{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: For now we only segment the liver\n",
    "- **Figure out why the train and validation loss are so different. Plot looks weird.**\n",
    "- Make training more stable.\n",
    "- Then train it for 3-4 days.\n",
    "- Write function to run it on all images, output dice per scan and avg dice. Which images work well and which don’t?\n",
    "- Connected component post-processing.\n",
    "\n",
    "Inspiration for improvements: **nnU-Net: Self-adapting Framework\n",
    "for U-Net-Based Medical Image Segmentation** (https://arxiv.org/pdf/1809.10486.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import warnings\n",
    "from random import randint\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from multi_slice_viewer import multi_slice_viewer\n",
    "from IPython.display import clear_output\n",
    "import pickle \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "import h5py\n",
    "import ipywidgets\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, UpSampling3D, concatenate, Cropping3D, Reshape, BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12708321216887263758\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1788466411457899457\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1300828307029982900\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16324862534411263626\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11322644890\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13109609844691788338\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11322644890\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11366887316504700339\n",
      "physical_device_desc: \"device: 1, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5\"\n",
      "]\n",
      "True\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "# check if we use gpu or cpu\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "assert len(K.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task03_liver dir in same directory as notebook\n",
    "data_path = './Task03_Liver/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about dataset in json file\n",
    "with open(data_path + 'dataset.json') as f:\n",
    "    d = json.load(f)   \n",
    "    \n",
    "    # paths to training set images with label\n",
    "    train_paths = d['training']\n",
    "    \n",
    "    # paths to testset images with label\n",
    "    test_paths = d['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to data dir \n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train set as SITK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generators that hold sitk images, not much memory usage..\n",
    "train_imgs = (sitk.ReadImage(train_instance['image']) for train_instance in train_paths)\n",
    "train_lbls = (sitk.ReadImage(train_instance['label']) for train_instance in train_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Images do not have the same spacings. We will first resample. For this we need the spacings in the SITK images. Note that when converting sitk to numpy the z axis is placed at the front. Spacings in order: (x, y, z), numpy image: (z, x, y)\n",
    "\n",
    "Resample to 1mm x 1mm x 1mm resolution => images should have different sizes (not all 512 x 512 x N anymore). \n",
    "For example, when the image has a shape of (512, 512, 74) and a spacing of (0.75, 0.75, 2),\n",
    "you can calculate how wide the image is along the x-axis: 512 * 0.75 mm = 384 mm. As a tip, look for “scipy zoom”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load one image and label at a time\n",
    "def resample_and_save(sitk_gen, order, filename, length):     \n",
    "    print('Storing resampled dataset at {}.'.format(os.path.join(os.getcwd(), filename)))    \n",
    "    i = 0\n",
    "    \n",
    "    # make a h5py file to store images as numpy in\n",
    "    with h5py.File(filename, 'w') as f: \n",
    "        for img in tqdm(sitk_gen, total=length):             \n",
    "#             print('Image index: {}.'.format(i))\n",
    "            \n",
    "            # get the spacing\n",
    "            spacing = img.GetSpacing()\n",
    "            # change order\n",
    "            x, y, z = spacing  \n",
    "            spacing = (z, x, y)      # order: (z, x, y)\n",
    "\n",
    "            # convert to numpy\n",
    "            np_img = sitk.GetArrayFromImage(img)\n",
    "\n",
    "            # apply zoom, order 3 for images\n",
    "            np_img_re = zoom(np_img, spacing, order=order)\n",
    "            \n",
    "#             print(\"Before: {}, after: {}\".format(np_img.shape, np_img_re.shape))\n",
    "\n",
    "            # save the resamples img/label\n",
    "            dataset = f.create_dataset(str(i), data=np_img_re)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use order 3 interpolation for imgs, order 1 \n",
    "resample_and_save(train_imgs, order=3, filename='resampled_train_imgs.h5py', length=len(train_paths))\n",
    "resample_and_save(train_lbls, order=0, filename='resampled_train_lbls.h5py', length=len(train_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if you saved the resampled images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home1/tdado/ISMI_project/Task03_Liver\n"
     ]
    }
   ],
   "source": [
    "# change to data dir\n",
    "data_path = './Task03_Liver/'\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f879aa2465b45c69faf8786477111b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=131), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aee75b8bd4f4f2da9ea58faa785d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=131), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load from disk\n",
    "np_train_imgs = []\n",
    "np_train_lbls = []\n",
    "\n",
    "with h5py.File('resampled_train_imgs.h5py', 'r') as f: \n",
    "    for img in tqdm(f):\n",
    "        dset = f[img]\n",
    "        np_train_imgs.append(dset[:])\n",
    "    \n",
    "with h5py.File('resampled_train_lbls.h5py', 'r') as f: \n",
    "    for lbl in tqdm(f):\n",
    "        dset = f[lbl]     \n",
    "        np_train_lbls.append(dset[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the labels binary\n",
    "For now we will focus on only on segmentation of the liver. Set the cancer labels to liver labels. Remove this line if you want to segment cancer aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_lbls = [np.where(lbl != 2, lbl, 1) for lbl in np_train_lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we have imbalances in our data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the labels of train images\n",
    "sums = np.zeros(3)\n",
    "for lbs in np_train_lbls:\n",
    "    labels, counts = np.unique(lbs, return_counts=True)\n",
    "    \n",
    "    # if there are only 2 labels\n",
    "    if len(counts) == 2:\n",
    "        sums[:2]+=counts\n",
    "    else:\n",
    "        sums+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.72% background, 2.28% liver, 0.00% cancer.\n"
     ]
    }
   ],
   "source": [
    "# print percentages of voxels.\n",
    "total = sum(sums)\n",
    "print(\"{:.2f}% background, {:.2f}% liver, {:.2f}% cancer.\".format(sums[0]/total*100, sums[1]/total*100, sums[2]/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  [  0.51166062  21.9396808 ]\n"
     ]
    }
   ],
   "source": [
    "weights = sum(sums[:2]) / (2.0 * sums[:2])\n",
    "print(\"class weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None: \n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial', overlay_1=self.lbls[i], overlay_1_thres=1, \n",
    "                   overlay_2=self.lbls[i], overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)\n",
    "        else:\n",
    "            plt.rcParams['figure.figsize'] = [8, 8]\n",
    "            multi_slice_viewer(self.imgs[i], view='axial')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 images in val set\n",
      "105 images in train set\n"
     ]
    }
   ],
   "source": [
    "# make a small data set of training images, as numpy\n",
    "validation_percent = 0.2 # coefficient to define validation dataset (value between 0 and 1)\n",
    "n_validation_imgs = int(validation_percent * len(np_train_imgs))\n",
    "\n",
    "val_set   = DataSet(np_train_imgs[:n_validation_imgs], np_train_lbls[:n_validation_imgs])\n",
    "train_set = DataSet(np_train_imgs[n_validation_imgs:], np_train_lbls[n_validation_imgs:])\n",
    "\n",
    "print('{} images in val set'.format(val_set.get_lenght()))\n",
    "print('{} images in train set'.format(train_set.get_lenght()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch extractor\n",
    "We re-use the patch extractor from assignment 7, but modify it to get 3D patches from a 3D image.\n",
    "We can add augmentations later in the patch extractor. Note the extra dimension in the shape of patch_out and target_out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, fromLiver):\n",
    "        self.patch_size = patch_size \n",
    "        self.fromLiver = fromLiver\n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a 3D patch of patch_size from 3D input image, along with corresponding 3D label map.\n",
    "        Pick random location of the patch inside the image. The point is at the center of the patch.\n",
    "        We first pad the image to not go out of bounds when extracting the patch.\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # size of patch in each dimension\n",
    "        pz, px, py = self.patch_size\n",
    "        \n",
    "#         print('Patch_size: {}'.format(patch_size))\n",
    "#         print('Image_size: {}'.format(image.shape))\n",
    "\n",
    "        # pad with the min value in the image\n",
    "        min_val = np.min(image)\n",
    "        \n",
    "        # pad with half the patch size, I assume even patch size\n",
    "        padded_img = np.pad(image, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant', constant_values=min_val)\n",
    "        padded_lbl = np.pad(label, ((pz//2, pz//2), (px//2, px//2), (py//2, py//2)), 'constant')\n",
    "        \n",
    "#         print('Padded_size: {}'.format(padded_img.shape))\n",
    "\n",
    "        # centre of the patch: a random point from the liver in the non padded image\n",
    "        if self.fromLiver:\n",
    "            # getting the liver labeled points\n",
    "            liver_ind = np.argwhere(label == 1)  \n",
    "            \n",
    "            # get a random point from the liver labeled points\n",
    "            r = randint(0, len(liver_ind))\n",
    "            z = liver_ind[r][0]\n",
    "            x = liver_ind[r][1]\n",
    "            y = liver_ind[r][2]\n",
    "            \n",
    "        # centre of the patch: a random location in the non padded image    \n",
    "        else:\n",
    "            dims = image.shape\n",
    "            z = randint(0, dims[0]) \n",
    "            x = randint(0, dims[1]) \n",
    "            y = randint(0, dims[2])   \n",
    "            \n",
    "        # z, x, y is the left bottom corner of the patch in the padded image (index shift with pad size)     \n",
    "        # take a patch, with the random point at the center in the padded img\n",
    "        patch  = padded_img[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "        target = padded_lbl[z:z+pz, x:x+px, y:y+py].reshape(pz, px, py, 1)\n",
    "\n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 156, 156, 1)\n",
      "(156, 156, 156, 1)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"800\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get an image and a label from our train set\n",
    "image = train_set.imgs[0]\n",
    "label = train_set.lbls[0]\n",
    "\n",
    "# test PatchExtractor\n",
    "patch_size = (156, 156, 156)\n",
    "patch_extractor = PatchExtractor(patch_size=patch_size, fromLiver=True)\n",
    "\n",
    "# lets check some patches\n",
    "patch, target = patch_extractor.get_patch(image, label)\n",
    "\n",
    "print(patch.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# show patch\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(patch.reshape(patch_size), view='axial', overlay_1=target.reshape(patch_size), overlay_1_thres=1, \n",
    "                   overlay_2=target.reshape(patch_size), overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch creator\n",
    "Lets also reuse the batch creator from assignment 7. We are going to use valid convolutions, which means the output of our network will be smaller than the input. The purpose of this batchcreator is the make batches consisting of patches with their corresponding labels (for the network to train on). Since a UNet with valid convolutions has a smaller output than input, we need to crop the label based on the target size aswell. And labels should be in onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single (batch of?) patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 1))  # 1 channel\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 2)) # one-hot encoding with 2 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs))                   # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index]       # get image and segmentation map\n",
    "            \n",
    "            # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "            img = np.clip(img, -1000, 3000)\n",
    "            img = (img - np.min(img)) / np.ptp(img)     \n",
    "            \n",
    "            # get a patch with corresponding labels from the patch extractor\n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl)   \n",
    "            \n",
    "            # crop labels based on target_size           \n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2    \n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            pd = (self.patch_extractor.patch_size[2] - self.target_size[2]) // 2\n",
    "            \n",
    "            # take the cropped patch, it contains labels with values 0,1,2\n",
    "            cropped_patch = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1], pd:pd+self.target_size[2]].squeeze() \n",
    "            \n",
    "            # instead of 0,1,2 label values we want categorical/onehot => 0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]\n",
    "            onehot = to_categorical(cropped_patch, num_classes=2)\n",
    "            \n",
    "            x_data[i, :, :, :, :] = patch_img\n",
    "            y_data[i, :, :, :, :] = onehot\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet Model\n",
    "Start with this model, we can adapt this later if needed. Build like the net from: \n",
    "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make block of two convolve3D's\n",
    "def unet_block(inputs, n_filters, padding, up_conv=False, batchnorm=False):\n",
    "    # 3d convolve, 32 3x3x3 filters \n",
    "    c1 = Conv3D(n_filters, (3,3,3), activation='relu', padding=padding)(inputs)\n",
    "    if batchnorm:\n",
    "        c1 = BatchNormalization()(c1)\n",
    "    \n",
    "    # up conv (normal conv in the expanding path) has same number of filters twice\n",
    "    if up_conv:\n",
    "        c2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "    else:          # normal convs have twice the filters in the second conv\n",
    "        c2 = Conv3D(n_filters*2, (3, 3, 3), activation='relu', padding=padding)(c1)\n",
    "        \n",
    "    if batchnorm:\n",
    "        c2 = BatchNormalization()(c2)\n",
    "    \n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Ozgun Cicek et al, 2016.\n",
    "def build_unet_3d(initial_filters, padding, batchnorm=False):\n",
    "    \n",
    "    ## CONTRACTING PATH\n",
    "    \n",
    "    # (spac_dim_1, space_dim_2, space_dim_3, channels)\n",
    "    inputs = Input(shape=(156, 156, 156, 1))\n",
    "\n",
    "    # First conv pool, 32 filters and 64 filters    \n",
    "    block_1    = unet_block(inputs, initial_filters, padding=padding, batchnorm=batchnorm) \n",
    "    max_pool_1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_1)  # 2×2×2 max pooling with strides two\n",
    "                                                                        # needs even spacial_dimensions as input\n",
    "    # second conv pool, 64 filters, 128 filters    \n",
    "    block_2    = unet_block(max_pool_1, initial_filters*2, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_2)\n",
    "    \n",
    "    # third conv pool, 128 filters, 256 filters    \n",
    "    block_3    = unet_block(max_pool_2, initial_filters*4, padding=padding, batchnorm=batchnorm)\n",
    "    max_pool_3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(block_3)\n",
    "    \n",
    "    # just a conv block without maxpooling, 256 filters and 512 filters\n",
    "    conv_4     = unet_block(max_pool_3, initial_filters*8, padding=padding, batchnorm=batchnorm)\n",
    "    \n",
    "    ## EXPANDING PATH   \n",
    "    \n",
    "    #TODO: check Conv3DTranspose correctly applied\n",
    "    \n",
    "    # round 1\n",
    "    up_conv_3  = Conv3DTranspose(16*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(conv_4)\n",
    "    crop_3     = Cropping3D(cropping=4)(block_3) \n",
    "    concat_3   = concatenate([crop_3, up_conv_3])  \n",
    "    up_block_3 = unet_block(concat_3, 8*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 2\n",
    "    up_conv_2  = Conv3DTranspose(8*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_3) \n",
    "    crop_2     = Cropping3D(cropping=16)(block_2) \n",
    "    concat_2   = concatenate([crop_2, up_conv_2])  \n",
    "    up_block_2 = unet_block(concat_2, 4*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # round 3\n",
    "    up_conv_1  = Conv3DTranspose(4*initial_filters, (2, 2, 2), strides=(2, 2, 2), padding=padding)(up_block_2) \n",
    "    crop_1     = Cropping3D(cropping=40)(block_1) \n",
    "    concat_1   = concatenate([crop_1, up_conv_1])  \n",
    "    up_block_1 = unet_block(concat_1, 2*initial_filters, padding, up_conv=True, batchnorm=batchnorm)\n",
    "    \n",
    "    # finish with 1x1x1 conv, 3 filters, # labels, softmax or ReLU?\n",
    "    finish = Conv3D(2, (1,1,1), activation='softmax', padding=padding)(up_block_1)\n",
    "    \n",
    "    model = Model(inputs, finish) \n",
    "    print(model.summary(line_length=150))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tdado/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             (None, 156, 156, 156, 1)         0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)                                (None, 154, 154, 154, 32)        896               input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)                                (None, 152, 152, 152, 64)        55360             conv3d_1[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)                   (None, 76, 76, 76, 64)           0                 conv3d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)                                (None, 74, 74, 74, 64)           110656            max_pooling3d_1[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)                                (None, 72, 72, 72, 128)          221312            conv3d_3[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)                   (None, 36, 36, 36, 128)          0                 conv3d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)                                (None, 34, 34, 34, 128)          442496            max_pooling3d_2[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)                                (None, 32, 32, 32, 256)          884992            conv3d_5[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)                   (None, 16, 16, 16, 256)          0                 conv3d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)                                (None, 14, 14, 14, 256)          1769728           max_pooling3d_3[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)                                (None, 12, 12, 12, 512)          3539456           conv3d_7[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_1 (Cropping3D)                        (None, 24, 24, 24, 256)          0                 conv3d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTranspose)             (None, 24, 24, 24, 512)          2097664           conv3d_8[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)                      (None, 24, 24, 24, 768)          0                 cropping3d_1[0][0]                                \n",
      "                                                                                                    conv3d_transpose_1[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)                                (None, 22, 22, 22, 256)          5308672           concatenate_1[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)                               (None, 20, 20, 20, 256)          1769728           conv3d_9[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_2 (Cropping3D)                        (None, 40, 40, 40, 128)          0                 conv3d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTranspose)             (None, 40, 40, 40, 256)          524544            conv3d_10[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)                      (None, 40, 40, 40, 384)          0                 cropping3d_2[0][0]                                \n",
      "                                                                                                    conv3d_transpose_2[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)                               (None, 38, 38, 38, 128)          1327232           concatenate_2[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)                               (None, 36, 36, 36, 128)          442496            conv3d_11[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "cropping3d_3 (Cropping3D)                        (None, 72, 72, 72, 64)           0                 conv3d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTranspose)             (None, 72, 72, 72, 128)          131200            conv3d_12[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)                      (None, 72, 72, 72, 192)          0                 cropping3d_3[0][0]                                \n",
      "                                                                                                    conv3d_transpose_3[0][0]                          \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)                               (None, 70, 70, 70, 64)           331840            concatenate_3[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)                               (None, 68, 68, 68, 64)           110656            conv3d_13[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)                               (None, 68, 68, 68, 2)            130               conv3d_14[0][0]                                   \n",
      "======================================================================================================================================================\n",
      "Total params: 19,069,058\n",
      "Trainable params: 19,069,058\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "unet_3d = build_unet_3d(initial_filters=32, padding='valid', batchnorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch, d, h, w, channels)\n",
      "xdata has shape: (2, 156, 156, 156, 1)\n",
      "ydata has shape: (2, 68, 68, 68, 2)\n",
      "Occuring values in true labels: [ 0.  1.]\n",
      "Min of input: 0.0\n",
      "Max of input: 0.6857683062553406\n"
     ]
    }
   ],
   "source": [
    "# define parameters for the batch creator\n",
    "patch_size  = (156, 156, 156)  # isotropic patch size\n",
    "target_size = (68, 68, 68)     # output size, smaller since valid convolutions are used\n",
    "batch_size  = 2                # number of patches in a mini-batch, for segmentation 1 is fine, since the \n",
    "                               # output of the net is many thousands of values per patch, which all contribute to the loss\n",
    "\n",
    "# initialize patch generator and batch creator\n",
    "patch_generator       = PatchExtractor(patch_size, fromLiver=True)\n",
    "batch_generator_train = BatchCreator(patch_generator, train_set, target_size=target_size)\n",
    "batch_generator_val   = BatchCreator(patch_generator, val_set, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator_train.create_image_batch(batch_size)\n",
    "\n",
    "print(\"(batch, d, h, w, channels)\")\n",
    "print('xdata has shape: {}'.format(x_data.shape))\n",
    "print('ydata has shape: {}'.format(y_data.shape))\n",
    "print('Occuring values in true labels: {}'.format(np.unique(y_data)))\n",
    "print('Min of input: {}'.format(np.min(x_data)))\n",
    "print('Max of input: {}'.format(np.max(x_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a logger which saves the losses and saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    # logg losses, removed accs for now\n",
    "    def __init__(self, data_dir, model_name):  \n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')        \n",
    "        self.tr_losses = []  \n",
    "        self.val_losses = []      \n",
    "        self.best_val_loss = float(\"inf\") \n",
    "        self.best_model = None     \n",
    "       \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # add validation info\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.tr_losses.append(logs.get('loss'))\n",
    "        \n",
    "        # per 30 epochs, training loss should improve with 5 × 10−3\n",
    "        # else: learning rate was reduced by factor 5.\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.005)\n",
    "        \n",
    "        # Training termination if val loss improvement < 0.005 within last 60 epochs & lr < 0.000001\n",
    "        # min_delta = 0.005\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.005, patience=60, verbose=1, mode='auto', \n",
    "                                      baseline=None, restore_best_weights=False)\n",
    "        \n",
    "        self.plot()\n",
    "\n",
    "        # safe best model after epoch end\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('Best model saved as {}'.format(self.model_filename))\n",
    "         \n",
    "    def plot(self): \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 4))\n",
    "#         plt.ylim([0, 0.8])\n",
    "        n = len(self.val_losses) + 1         \n",
    "        plt.plot(range(1, n), self.tr_losses, label='train loss')         \n",
    "        plt.plot(range(1, n), self.val_losses, label='val loss')        \n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home1/tdado/ISMI_project/Task03_Liver\n"
     ]
    }
   ],
   "source": [
    "# make a data dir to store best model\n",
    "print(os.getcwd())\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to inline, I don't know how to plot in notebook mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "Dice loss seems to be a good pick for 3D segmentation with class inbalances. We have to look if this works like this.\n",
    "In our case y_pred is output of the softmax.(see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "\n",
    "$$ \\textbf{Dice loss} =  1 - \\frac{2 \\hspace{0.3em}|X \\cap Y|}{|X|+ |Y|} $$\n",
    "\n",
    "\n",
    "**For binary volumes of N voxels (Milletari et al., 2016, VNet):**\n",
    "\n",
    "$$ \\textbf{Dice loss} = 1 -\\frac{2 \\sum_{i}^{N} p_{i} g_{i}}{\\sum_{i}^{N} p_{i}^{2}+\\sum_{i}^{N} g_{i}^{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss as above\n",
    "def dice_loss_bv(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    G = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    return 1. - (2. * K.sum(P * G) + epsilon) / (K.sum(P**2.) + K.sum(G**2.) + epsilon) + keras.losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed in Milletari et al. [8] as a loss function, the 2-class variant of the Dice loss, denoted DL2, can be expressed as**\n",
    "\n",
    "$$\n",
    "\\mathrm{DL}_{2}=1-\\frac{\\sum_{n=1}^{N} p_{n} r_{n}+\\epsilon}{\\sum_{n=1}^{N} p_{n}+r_{n}+\\epsilon}-\\frac{\\sum_{n=1}^{N}\\left(1-p_{n}\\right)\\left(1-r_{n}\\right)+\\epsilon}{\\sum_{n=1}^{N} 2-p_{n}-r_{n}+\\epsilon}\n",
    "$$\n",
    "\n",
    "Sudre, Carole H., et al. \"Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.\" https://arxiv.org/pdf/1707.03237.pdf\n",
    "\n",
    "\n",
    "**Let R be the reference foreground segmentation\n",
    "(gold standard) with voxel values $r_n$, and P the predicted probabilistic map for the foreground label over N image elements $p_n$, with the background class probability being 1 − P. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, epsilon=1e-6):\n",
    "    ''' \n",
    "    Dice loss calculation in a binary classification (foreground vs. background) formulation.\n",
    "    Assumes the channels_last format.\n",
    "    y_true: One hot encoding of ground truth\n",
    "    y_pred: Network output, must sum to 1 over c channel (such as after softmax) \n",
    "    '''\n",
    "    # for every voxel of the prediction the probabililty of being foreground (liver\n",
    "    P = K.sum(y_pred * [0., 1.], axis=-1)\n",
    "    \n",
    "    # for every voxel of the groundtruth the label (0: background, 1: foreground)\n",
    "    R = K.sum(y_true * [0., 1.], axis=-1)\n",
    "    \n",
    "    a = K.sum(P * R) + epsilon\n",
    "    b = K.sum(P + R) + epsilon\n",
    "    c = K.sum((1 - P) * (1 - R)) + epsilon \n",
    "    d = K.sum((2 - P - R)) + epsilon\n",
    "    \n",
    "    return (1 - a/b - c/d) + keras.losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred, weights=weights):\n",
    "    '''# 97.98% background, 1.91% liver, 0.11% cancer.\n",
    "        weights = sum(sums) / (3.0 * sums)\n",
    "        print(\"class weights: \", weights)'''\n",
    "#     weights = K.variable(weights)\n",
    "#     # scale predictions so that the class probas of each sample sum to 1\n",
    "#     y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "#     # clip to prevent NaN's and Inf's\n",
    "#     y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "#     # calc\n",
    "#     loss = y_true * K.log(y_pred) * weights\n",
    "#     loss = -K.sum(loss, -1)\n",
    "    return keras.losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.51166062  21.9396808 ]\n",
      "<class 'numpy.ndarray'>\n",
      "{0: 0.51166062136303914, 1: 21.939680804011854}\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(type(weights))\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we define parameters, compile the model and train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model use multiple gpus, I think batchsize should be a multiple of #gpu's\n",
    "model = multi_gpu_model(unet_3d, gpus=2)\n",
    "# model = unet_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 10**-3\n",
    "optimizer       = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# loss            = \n",
    "steps_per_epoch = 100\n",
    "epochs          = 100\n",
    "logger          = Logger(data_dir, '3D-UNet-29-05')\n",
    "\n",
    "image_generator_train = batch_generator_train.get_image_generator(batch_size)\n",
    "image_generator_val   = batch_generator_val.get_image_generator(batch_size)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b547aced320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      " 43/100 [===========>..................] - ETA: 11:30 - loss: 9.4992"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit_generator(generator=image_generator_train, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=image_generator_val,\n",
    "                    verbose=1,\n",
    "                    validation_steps=20,\n",
    "                    callbacks=[logger],\n",
    "                   class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the full segmentation map\n",
    "Like this but then 3D:\n",
    "\n",
    "![seg_diagram.png](seg_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(image, patch_size, target_size):\n",
    "    \"\"\"\n",
    "    Adding the red border (see example above) to the image. Which is needed for when we don't have full context. \n",
    "    Pad with lowest occuring values.\n",
    "    image       : the input image (as numpy)\n",
    "    patch_size  : patch_size of the input for the UNet\n",
    "    target_size : output size of the model, needed to calculate how much to padd in each dimension. \n",
    "    \"\"\"\n",
    "    z, y, x = patch_size\n",
    "    \n",
    "    # pad with min value from image, always safe\n",
    "    min_val = np.min(image)\n",
    "    \n",
    "    # size of padding for each dimension\n",
    "    pad_z = (z - target_size[0]) // 2\n",
    "    pad_x = (x - target_size[1]) // 2\n",
    "    pad_y = (y - target_size[2]) // 2\n",
    "    \n",
    "    # pad with a tuple for how much on each side for every dimension\n",
    "    padded_input = np.pad(image, ((pad_z, pad_z), (pad_x, pad_x), (pad_y, pad_y)), 'constant', constant_values=min_val)\n",
    "    \n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_segmentation(model, image, target_size, patch_size):\n",
    "    \"\"\"\n",
    "    Give a full segmentation map (same size as input_image) using the model. \n",
    "    model       : the model to do the prediction\n",
    "    image       : the input image (as numpy)\n",
    "    target_size : output size of the model (since we use valid convutions the output gets smaller)\n",
    "    patch_size: : the size of the patch that is put into the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # clip values outside [-1000, 3000] and normalize image intensity to range [0., 1.]      \n",
    "    image = np.clip(image, -1000, 3000)\n",
    "    image = (image - np.min(image)) / np.ptp(image)    \n",
    "    \n",
    "    # pad the input image:\n",
    "    pad_img = padding(image, patch_size, target_size)  \n",
    "\n",
    "    print(\"Image size: {}\".format(image.shape))\n",
    "    print(\"Padded image size: {}\".format(pad_img.shape))\n",
    "    \n",
    "    dims = image.shape\n",
    "    # how many times target size fits in a dimension \n",
    "    pz = dims[0] // target_size[0] \n",
    "    px = dims[1] // target_size[1] \n",
    "    py = dims[2] // target_size[2] \n",
    "    \n",
    "    # segmentation map, same size as input image\n",
    "    segmentation = np.zeros(image.shape)   \n",
    "    \n",
    "    for z in range(pz, -1, -1):         \n",
    "        for x in range(px, -1, -1):\n",
    "            for y in range(py, -1, -1):  \n",
    "                \n",
    "                # shift starting point with target_size\n",
    "                start_z = z * target_size[0]\n",
    "                start_x = x * target_size[1]\n",
    "                start_y = y * target_size[2]\n",
    "                \n",
    "                # if the patch does not fit:\n",
    "                if start_z + patch_size[0] > pad_img.shape[0]:\n",
    "                    start_z = pad_img.shape[0] - patch_size[0]\n",
    "                if start_x + patch_size[1] > pad_img.shape[1]:\n",
    "                    start_x = pad_img.shape[1] - patch_size[1]\n",
    "                if start_y + patch_size[2] > pad_img.shape[2]:\n",
    "                    start_y = pad_img.shape[2] - patch_size[2]\n",
    "                \n",
    "                # Get patch: shift with target_size, take patch_size                \n",
    "                patch = pad_img[start_z:start_z + patch_size[0], \n",
    "                                start_x:start_x + patch_size[1], \n",
    "                                start_y:start_y + patch_size[2]]     \n",
    "\n",
    "                # Reshape for u-net and make prediction:\n",
    "                patch = np.reshape(patch, (1, patch_size[0], patch_size[1], patch_size[2], 1))\n",
    "                prediction = model.predict(patch)\n",
    "\n",
    "                # Put the prediction in segmentation map, shift with target_size, take target_size\n",
    "                segmentation[start_z:start_z + target_size[0], \n",
    "                             start_x:start_x + target_size[1], \n",
    "                             start_y:start_y + target_size[2]] = np.argmax(np.squeeze(prediction), axis=3)\n",
    "    \n",
    "    return segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an image and a label from the validation set\n",
    "image = val_set.imgs[3]\n",
    "label = val_set.lbls[3]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "unet_3d = load_model(os.path.join(data_dir, '3D-UNet-23-05' + '.h5'), custom_objects={'dice_loss': dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the segmentation map\n",
    "segmentation = predict_image_segmentation(unet_3d, image, target_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(segmentation, return_counts=True))\n",
    "print(np.unique(label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connected component analysis\n",
    "stats = sitk.LabelShapeStatisticsImageFilter()\n",
    "stats.Execute(sitk.ConnectedComponent(cleaned_thresh_img))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot slices\n",
    "%matplotlib notebook\n",
    "s = 125\n",
    "slice_img = image[s, :, :]\n",
    "slice_lbl = label[s, :, :]\n",
    "slice_seg = segmentation[s, :, :]\n",
    "\n",
    "masked_lbl = np.ma.masked_where(slice_lbl < 1, slice_lbl)\n",
    "masked_seg = np.ma.masked_where(slice_seg < 1, slice_seg)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1).set_title('Prediction')\n",
    "plt.imshow(slice_img, cmap='gray')\n",
    "if (np.any(masked_seg)):\n",
    "    plt.imshow(masked_seg, cmap='coolwarm', alpha = 0.75)\n",
    "\n",
    "plt.subplot(1,2,2).set_title('Ground truth')\n",
    "plt.imshow(slice_img, cmap='gray')\n",
    "plt.imshow(masked_lbl, cmap='coolwarm', alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show segmentation\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=segmentation, overlay_1_thres=1, \n",
    "                   overlay_2=segmentation, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth\n",
    "plt.rcParams['figure.figsize'] = [8, 8]            \n",
    "multi_slice_viewer(image, view='axial', overlay_1=label, overlay_1_thres=1, \n",
    "                   overlay_2=label, overlay_2_thres=2, overlay_2_cmap='coolwarm', overlay_2_alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
